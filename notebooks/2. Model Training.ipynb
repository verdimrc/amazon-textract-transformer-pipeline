{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Post-Processing Amazon Textract with Location-Aware Transformers**\n",
    "\n",
    "# Part 2: Data Consolidation and Model Training/Deployment\n",
    "\n",
    "> *This notebook works well with the `Python 3 (Data Science)` kernel on SageMaker Studio*\n",
    "\n",
    "In the [first notebook](1.%20Data%20Preparation.ipynb) we worked through preparing a corpus with Amazon Textract and labelling a small sample to highlight entities of interest.\n",
    "\n",
    "In this part 2, we'll consolidate the labelling results together with a pre-prepared augmentation set, and actually train and deploy a SageMaker model for word classification.\n",
    "\n",
    "First, as in the previous notebook, we'll start by importing the required libraries and loading configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-18 04:14:27,347 project [INFO] No PROJECT_ID variable found in environment: You'll need to call init('myprojectid')\n",
      "Working in bucket s3:\u001b[35m/\u001b[0m\u001b[35m/sagemaker-ap-southeast-1-111122223333/textract-transformers/\u001b[0m\n",
      "2022-01-18 04:14:29,281 project [INFO] Working in project 'ocr-transformers-demo'\n",
      "\u001b[1m<\u001b[0m\u001b[1;35mutil.project.ProjectSession\u001b[0m\u001b[1;39m(\u001b[0m\n",
      "\u001b[39m  \u001b[0m\u001b[33mproject_id\u001b[0m\u001b[39m=\u001b[0m\u001b[35mocr\u001b[0m\u001b[39m-transformers-demo,\u001b[0m\n",
      "\u001b[39m  \u001b[0m\u001b[33ma2i_review_flow_arn_param\u001b[0m\u001b[39m=\u001b[0m\u001b[35m/ocr-transformers-demo/config/\u001b[0m\u001b[95mHumanReviewFlowArn\u001b[0m\u001b[39m,\u001b[0m\n",
      "\u001b[39m  \u001b[0m\u001b[33mentity_config_param\u001b[0m\u001b[39m=\u001b[0m\u001b[35m/ocr-transformers-demo/config/\u001b[0m\u001b[95mEntityConfiguration\u001b[0m\u001b[39m,\u001b[0m\n",
      "\u001b[39m  \u001b[0m\u001b[33msagemaker_endpoint_name_param\u001b[0m\u001b[39m=\u001b[0m\u001b[35m/ocr-transformers-demo/config/\u001b[0m\u001b[95mSageMakerEndpointName\u001b[0m\u001b[39m,\u001b[0m\n",
      "\u001b[39m  \u001b[0m\u001b[33ma2i_execution_role_arn\u001b[0m\u001b[39m=\u001b[0m\u001b[35marn\u001b[0m\u001b[39m:aws:iam::\u001b[0m\u001b[1;36m111122223333\u001b[0m\u001b[39m:role/OCRPipelineDemo-ProcessingPipelineReviewStepProces-17SOGS2NSBCM3,\u001b[0m\n",
      "\u001b[39m  \u001b[0m\u001b[33mpipeline_input_bucket_name\u001b[0m\u001b[39m=\u001b[0m\u001b[35mocrpipelinedemo\u001b[0m\u001b[39m-pipelineinputbucket350ea1ae-f9kjwhjfj1bz,\u001b[0m\n",
      "\u001b[39m  \u001b[0m\u001b[33mpipeline_sfn_arn\u001b[0m\u001b[39m=\u001b[0m\u001b[35marn\u001b[0m\u001b[39m:aws:states:ap-southeast-\u001b[0m\u001b[1;92m1:4845\u001b[0m\u001b[39m97657167:stateMachine:ProcessingPipelineProcessingPipelineStateMachine91667981-5A6DQRRzn6Kl,\u001b[0m\n",
      "\u001b[39m  \u001b[0m\u001b[33mplain_textract_sfn_arn\u001b[0m\u001b[39m=\u001b[0m\u001b[35marn\u001b[0m\u001b[39m:aws:states:ap-southeast-\u001b[0m\u001b[1;92m1:4845\u001b[0m\u001b[39m97657167:stateMachine:ProcessingPipelineOCRStepTextractStateMachine1970CD25-78JIbhatroBp,\u001b[0m\n",
      "\u001b[39m  \u001b[0m\u001b[33mpipeline_reviews_bucket_name\u001b[0m\u001b[39m=\u001b[0m\u001b[35mocrpipelinedemo\u001b[0m\u001b[39m-processingpipelinehumanreviewsbuc-vszv3a21k5f2,\u001b[0m\n",
      "\u001b[39m  \u001b[0m\u001b[33msm_image_build_role\u001b[0m\u001b[39m=\u001b[0m\u001b[35mOCRPipelineDemo\u001b[0m\u001b[39m-AnnotationInfraSMImageBuildRole8DB-1COB4NPSEFB73\u001b[0m\n",
      "\u001b[1;39m)\u001b[0m\u001b[39m at \u001b[0m\u001b[1;36m0x7f682201f0a0\u001b[0m\u001b[1m>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Python Built-Ins:\n",
    "from datetime import datetime\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3  # AWS SDK for Python\n",
    "import sagemaker\n",
    "\n",
    "# Local Dependencies:\n",
    "import util\n",
    "from sagemaker.huggingface import HuggingFace as HuggingFaceEstimator\n",
    "from tqdm.notebook import tqdm  # Progress bars\n",
    "\n",
    "try:\n",
    "    from loguru import logger\n",
    "except:\n",
    "    from logging import getLogger\n",
    "\n",
    "    logger = getLogger()\n",
    "\n",
    "try:\n",
    "    import rich\n",
    "\n",
    "    rich.reconfigure(force_terminal=True, force_jupyter=False)\n",
    "    rich.pretty.install()\n",
    "    print = rich.get_console().out\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Manual configuration (check this matches notebook 1):\n",
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "bucket_prefix = \"textract-transformers/\"\n",
    "print(f\"Working in bucket s3://{bucket_name}/{bucket_prefix}\")\n",
    "config = util.project.init(\"ocr-transformers-demo\")\n",
    "print(config)\n",
    "\n",
    "# Field configuration saved from first notebook:\n",
    "with open(\"data/field-config.json\", \"r\") as f:\n",
    "    fields = [\n",
    "        util.postproc.config.FieldConfiguration.from_dict(cfg) for cfg in json.loads(f.read())\n",
    "    ]\n",
    "entity_classes = [f.name for f in fields]\n",
    "\n",
    "# S3 URIs as per first notebook:\n",
    "imgs_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/imgs-clean\"\n",
    "textract_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/textracted\"\n",
    "annotations_base_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/annotations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Consolidation\n",
    "\n",
    "To construct a training set, we'll typically need to consolidate the results of multiple SageMaker Ground Truth labelling jobs: Perhaps because the work was split up into more manageable chunks - or maybe because additional review/adjustment jobs were run to improve label quality.\n",
    "\n",
    "First, we'll download the output folders of all our labelling jobs to the local `data/annotations` folder: (The code here assumes you configured the same `annotations_base_s3uri` output folder for each job in SMGT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync --quiet $annotations_base_s3uri ./data/annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside this folder, you'll find some **pre-annotated augmentation data** provided for you already (in the `augmentation-` subfolders). These datasets are not especially large or externally useful, but will help you train a better model without too much (or even any!) manual annotation effort.\n",
    "\n",
    "▶️ **Edit** the `include_jobs` line below to control which datasets (pre-provided and your own) will be included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got \u001b[1;36m2\u001b[0m annotated manifests:\n",
      "data/annotations/augmentation-\u001b[1;36m1\u001b[0m/manifests/output/output.manifest\n",
      "data/annotations/augmentation-\u001b[1;36m2\u001b[0m/manifests/output/output.manifest\n"
     ]
    }
   ],
   "source": [
    "include_jobs = [\n",
    "    \"augmentation-1\",\n",
    "    \"augmentation-2\",\n",
    "    # TODO: Adjust the below to match the labelling jobs you created, or comment out if you didn't:\n",
    "    # \"cfpb-boxes-1\",\n",
    "]\n",
    "\n",
    "\n",
    "source_manifests = []\n",
    "for job_name in sorted(\n",
    "    filter(lambda n: os.path.isdir(f\"data/annotations/{n}\"), os.listdir(\"data/annotations\"))\n",
    "):\n",
    "    if job_name not in include_jobs:\n",
    "        logger.warning(f\"Skipping {job_name} (not in include_jobs list)\")\n",
    "        continue\n",
    "    job_manifest_path = f\"data/annotations/{job_name}/manifests/output/output.manifest\"\n",
    "    if not os.path.isfile(job_manifest_path):\n",
    "        raise RuntimeError(f\"Could not find job output manifest {job_manifest_path}\")\n",
    "    source_manifests.append({\"job_name\": job_name, \"manifest_path\": job_manifest_path})\n",
    "\n",
    "print(f\"Got {len(source_manifests)} annotated manifests:\")\n",
    "print(\"\\n\".join(map(lambda o: o[\"manifest_path\"], source_manifests)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the results are downloaded, we're ready to consolidate the **output manifest files** from each one into a combined manifest file.\n",
    "\n",
    "Note that to combine multiple output manifests to a single dataset:\n",
    "\n",
    "- We need to ensure the labels are stored in the same attribute on every record (records use the labeling job name by default, which will be different between jobs).\n",
    "- If importing data collected from some other account (like the `augmentation-` sets), we'll need to **map the S3 URIs** to equivalent links on your own bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\n",
      "    \u001b[1m{\u001b[0m\n",
      "        \u001b[32m'job_name'\u001b[0m: \u001b[32m'augmentation-1'\u001b[0m,\n",
      "        \u001b[32m'manifest_path'\u001b[0m: \u001b[32m'data/annotations/augmentation-1/manifests/output/output.manifest'\u001b[0m\n",
      "    \u001b[1m}\u001b[0m,\n",
      "    \u001b[1m{\u001b[0m\n",
      "        \u001b[32m'job_name'\u001b[0m: \u001b[32m'augmentation-2'\u001b[0m,\n",
      "        \u001b[32m'manifest_path'\u001b[0m: \u001b[32m'data/annotations/augmentation-2/manifests/output/output.manifest'\u001b[0m\n",
      "    \u001b[1m}\u001b[0m\n",
      "\u001b[1m]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "source_manifests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/annotations/annotations-all.manifest.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50164777ab44c3f9ba4e7521b4e0d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Consolidating manifests...:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Annotations/labels will be standardized to this field on all records:\n",
    "standard_label_field = \"label\"\n",
    "\n",
    "# To import a manifest from somebody else, we of course need to map their bucket names and prefixes\n",
    "# to ours (and have equivalent files stored in the same locations after the mapping):\n",
    "BUCKET_MAPPINGS = {\"DOC-EXAMPLE-BUCKET\": bucket_name}\n",
    "PREFIX_MAPPINGS = {\"EXAMPLE-PREFIX/\": bucket_prefix}\n",
    "\n",
    "annotated_page_imgs = {}\n",
    "print(\"Writing data/annotations/annotations-all.manifest.jsonl\")\n",
    "with open(\"data/annotations/annotations-all.manifest.jsonl\", \"w\") as fout:\n",
    "    for source in tqdm(source_manifests, desc=\"Consolidating manifests...\"):\n",
    "        with open(source[\"manifest_path\"], \"r\") as fin:\n",
    "            for line in filter(lambda l: l, fin):\n",
    "                obj = json.loads(line)\n",
    "\n",
    "                # Import refs by applying BUCKET_MAPPINGS and PREFIX_MAPPINGS:\n",
    "                for k in filter(lambda k: k.endswith(\"-ref\"), obj.keys()):\n",
    "                    if not obj[k].lower().startswith(\"s3://\"):\n",
    "                        raise RuntimeError(\n",
    "                            \"Attr {} ends with -ref but does not start with 's3://'\\n{}\".format(\n",
    "                                k, obj\n",
    "                            )\n",
    "                        )\n",
    "                    obj_bucket, _, obj_key = obj[k][len(\"s3://\") :].partition(\"/\")\n",
    "                    obj_bucket = BUCKET_MAPPINGS.get(obj_bucket, obj_bucket)\n",
    "                    for old_prefix in PREFIX_MAPPINGS:\n",
    "                        if obj_key.startswith(old_prefix):\n",
    "                            obj_key = PREFIX_MAPPINGS[old_prefix] + obj_key[len(old_prefix) :]\n",
    "                    obj[k] = f\"s3://{obj_bucket}/{obj_key}\"\n",
    "\n",
    "                # Find the job output field:\n",
    "                if source[\"job_name\"] in obj:\n",
    "                    source_label_attr = source[\"job_name\"]\n",
    "                elif standard_label_field in obj:\n",
    "                    source_label_attr = standard_label_field\n",
    "                else:\n",
    "                    raise RuntimeError(\n",
    "                        \"Couldn't find label field for entry in {}:\\n{}\".format(\n",
    "                            source[\"job_name\"],\n",
    "                            obj,\n",
    "                        )\n",
    "                    )\n",
    "                # Rename to standard:\n",
    "                obj[standard_label_field] = obj.pop(source_label_attr)\n",
    "                source_meta_attr = f\"{source_label_attr}-metadata\"\n",
    "                if source_meta_attr in obj:\n",
    "                    obj[f\"{standard_label_field}-metadata\"] = obj.pop(source_meta_attr)\n",
    "                # Write to output manifest:\n",
    "                fout.write(json.dumps(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training and test sets\n",
    "\n",
    "To get some insight on how well our model is generalizing to real-world data, we'll need to reserve some annotated data as a testing/validation set.\n",
    "\n",
    "Below, we randomly partition the data into training and test sets and then upload the two manifests to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-18 04:38:28,618 root [INFO] Reading data/annotations/annotations-all.manifest.jsonl\n",
      "2022-01-18 04:38:28,624 root [INFO] Shuffling records\n",
      "2022-01-18 04:38:28,625 root [INFO] Writing 90 records to data/annotations/annotations-train.manifest.jsonl\n",
      "2022-01-18 04:38:28,627 root [INFO] Writing 10 records to data/annotations/annotations-test.manifest.jsonl\n"
     ]
    }
   ],
   "source": [
    "def split_manifest(f_in, f_train, f_test, train_pct=0.9, random_seed=1337):\n",
    "    logger.info(f\"Reading {f_in}\")\n",
    "    with open(f_in, \"r\") as fin:\n",
    "        lines = [l for l in filter(lambda l: l, fin)]\n",
    "    logger.info(f\"Shuffling records\")\n",
    "    random.Random(random_seed).shuffle(lines)\n",
    "    n_train = round(len(lines) * train_pct)\n",
    "\n",
    "    with open(f_train, \"w\") as ftrain:\n",
    "        logger.info(f\"Writing {n_train} records to {f_train}\")\n",
    "        for l in lines[:n_train]:\n",
    "            ftrain.write(l)\n",
    "    with open(f_test, \"w\") as ftest:\n",
    "        logger.info(f\"Writing {len(lines) - n_train} records to {f_test}\")\n",
    "        for l in lines[n_train:]:\n",
    "            ftest.write(l)\n",
    "\n",
    "\n",
    "split_manifest(\n",
    "    \"data/annotations/annotations-all.manifest.jsonl\",\n",
    "    \"data/annotations/annotations-train.manifest.jsonl\",\n",
    "    \"data/annotations/annotations-test.manifest.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/annotations/annotations-train.manifest.jsonl to s3://sagemaker-ap-southeast-1-111122223333/textract-transformers/data/annotations/annotations-train.manifest.jsonl\n",
      "upload: data/annotations/annotations-test.manifest.jsonl to s3://sagemaker-ap-southeast-1-111122223333/textract-transformers/data/annotations/annotations-test.manifest.jsonl\n"
     ]
    }
   ],
   "source": [
    "train_manifest_s3uri = (\n",
    "    f\"s3://{bucket_name}/{bucket_prefix}data/annotations/annotations-train.manifest.jsonl\"\n",
    ")\n",
    "!aws s3 cp data/annotations/annotations-train.manifest.jsonl $train_manifest_s3uri\n",
    "\n",
    "test_manifest_s3uri = (\n",
    "    f\"s3://{bucket_name}/{bucket_prefix}data/annotations/annotations-test.manifest.jsonl\"\n",
    ")\n",
    "!aws s3 cp data/annotations/annotations-test.manifest.jsonl $test_manifest_s3uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data\n",
    "\n",
    "Before training the model, we'll sense-check the data by plotting a few examples.\n",
    "\n",
    "The utility function below will overlay the page image with the annotated bounding boxes, the locations of `WORD` blocks detected from the Amazon Textract results, and the resulting classification of individual Textract `WORD`s.\n",
    "\n",
    "> ⏰ If you Textracted a large number of documents and haven't previously synced them to the notebook, the initial download here may take a few minutes to complete. For our sample set of 120, typically only ~20s is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 107 ms, sys: 26.7 ms, total: 134 ms\n",
      "Wall time: 5.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!aws s3 sync --quiet $textract_s3uri ./data/textracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Note:** For the interactive visualization widgets in this notebook to work correctly, you'll need the [IPyWidgets extension for JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html).\n",
    ">\n",
    "> On [SageMaker Studio](https://aws.amazon.com/sagemaker/studio/), this should be installed by default.\n",
    ">\n",
    "> On the classic [SageMaker Notebook Instances](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html) though, you'll need to install the `@jupyter-widgets/jupyterlab-manager` extension (from `Settings > Extension Manager`, or using a [lifecycle configuration](https://docs.aws.amazon.com/sagemaker/latest/dg/notebook-lifecycle-config.html) similar to [this sample](https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/tree/master/scripts/install-lab-extension)) - or just use plain `Jupyter` instead of `JupyterLab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e095ea5d36fa41ba96a14e4257e27828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Example:', max=9), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m<\u001b[0m\u001b[1;95mfunction\u001b[0m\u001b[39m draw_from_manifest_items.<locals\u001b[0m\u001b[1m>\u001b[0m.draw at \u001b[1;36m0x7f6815a2d3f0\u001b[0m>\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/annotations/annotations-test.manifest.jsonl\", \"r\") as fman:\n",
    "    test_examples = [json.loads(l) for l in filter(lambda l: l, fman)]\n",
    "\n",
    "util.viz.draw_from_manifest_items(\n",
    "    test_examples,\n",
    "    standard_label_field,\n",
    "    entity_classes,\n",
    "    imgs_s3uri[len(\"s3://\") :].partition(\"/\")[2],\n",
    "    textract_s3key_prefix=textract_s3uri[len(\"s3://\") :].partition(\"/\")[2],\n",
    "    imgs_local_prefix=\"data/imgs-clean\",\n",
    "    textract_local_prefix=\"data/textracted\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-supervised pre-training\n",
    "\n",
    "In many cases, businesses have a great deal more relevant *unlabelled* data available in addition to the manually labeled dataset. For example, you might have many more historical documents available (with OCR results already, or able to be processed with Amazon Textract) than you're reasonably able to annotate entities on - just as we do in this example!\n",
    "\n",
    "Large-scale language models like LayoutLM are typically **pre-trained** to unlabelled data in a **self-supervised** pattern: Teaching the model to predict some implicit task in the data like, for example, masking a few words on the page and predicting what words should go in the gaps.\n",
    "\n",
    "This pre-training doesn't directly teach the model to perform the target task (classifying entities), but forces the core of the model to learn intrinsic patterns in the data. When we then replace the output layers and **fine-tune** towards the target task with human-labelled data, the model is able to learn the target task more effectively.\n",
    "\n",
    "**In this example, pre-training is optional**:\n",
    "\n",
    "- By default, for speed, the configuration below will use a public pre-trained model from the [Hugging Face Transformers model repository](https://huggingface.co/models?search=layoutlm). This allows us to focus immediately on fine-tuning to our task; but also means accuracy may be degraded if our documents are very different from the original corpus the model was trained on.\n",
    "- Alternatively, set `pretrain = True` below to *further* pre-train this same base public model on your own Textracted documents first.\n",
    "\n",
    "Pre-training more likely to be valuable where you have a broader range of data available than the core supervised/annotated dataset, and the language/layouts used in your domain are unusual or specicalized. If you followed through [Notebook 1](1.%20Data%20Preparation.ipynb) with the default settings to Amazon Textract only a small sample of the documents, you may like to go back, increase `N_DOCS_KEPT`, and Textract some more of the source documents first.\n",
    "\n",
    "> ⚠️ **Note:** Refer to the [Amazon SageMaker Pricing Page](https://aws.amazon.com/sagemaker/pricing/) for up-to-date guidance before running large pre-training jobs.\n",
    ">\n",
    "> In our tests at the time of writing:\n",
    ">\n",
    "> - Pre-training on only the 120 \"sample\" documents to 25 epochs took about 30 minutes on an `ml.p3.8xlarge` instance with per-device batch size 4\n",
    "> - Pre-training on a larger 500-document subset with the same infrastructure and settings took about an hour\n",
    "> - Although the observed effect on downstream (entity recognition) accuracy metrics was generally positive in either case, it was small compared to variation over random seed initializations in fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain = True  # Will run an additional pre-training job.\n",
    "# pretrain = False  # Uncomment to skip pre-training.\n",
    "\n",
    "pretrained_s3_uri = None  # Will be overwritten later if pretrain is enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For self-supervised pre-training, you can utilize the full available corpus of Textract-processed documents: Not just the subset of documents and pages you have annotations for. Reserving some documents for validation is still a good idea though, to understand if and when the model starts to over-fit.\n",
    "\n",
    "Arguably, including pages from the entity recognition validation dataset in pre-training constitutes [leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)): Because even though we're not including any information about the entity labels the NER model will predict, we're teaching the model information about patterns of content in the hold-out pages.\n",
    "\n",
    "Therefore, the below code takes a conservative view to avoid possibly over-estimating the added benefits of pre-training: Constructing manifests to route *any document with pages in the entity recognition validation set* to also be in the validation set for pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added \u001b[1;36m10\u001b[0m docs to pre-training validation set\n",
      "Added \u001b[1;36m110\u001b[0m docs to pre-training set\n"
     ]
    }
   ],
   "source": [
    "selfsup_train_manifest_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/docs-train.manifest.jsonl\"\n",
    "selfsup_val_manifest_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/docs-val.manifest.jsonl\"\n",
    "\n",
    "# To avoid information leakage, take the validation set = the set of all documents with *any* pages\n",
    "# mentioned in the validation set:\n",
    "val_textract_s3uris = set()\n",
    "with open(\"data/annotations/annotations-test.manifest.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        val_textract_s3uris.add(json.loads(line)[\"textract-ref\"])\n",
    "with open(\"data/docs-val.manifest.jsonl\", \"w\") as f:\n",
    "    for uri in val_textract_s3uris:\n",
    "        f.write(json.dumps({\"textract-ref\": uri}) + \"\\n\")\n",
    "print(f\"Added {len(val_textract_s3uris)} docs to pre-training validation set\")\n",
    "\n",
    "# Any Textracted docs not mentioned in validation can go to training:\n",
    "train_textract_s3uris = set()\n",
    "with open(\"data/textracted-all.manifest.jsonl\", \"r\") as fner:\n",
    "    with open(\"data/docs-train.manifest.jsonl\", \"w\") as f:\n",
    "        for line in fner:\n",
    "            uri = json.loads(line)[\"textract-ref\"]\n",
    "            if (uri in val_textract_s3uris) or (uri in train_textract_s3uris):\n",
    "                continue\n",
    "            else:\n",
    "                train_textract_s3uris.add(uri)\n",
    "                f.write(json.dumps({\"textract-ref\": uri}) + \"\\n\")\n",
    "print(f\"Added {len(train_textract_s3uris)} docs to pre-training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/docs-train.manifest.jsonl to s3://sagemaker-ap-southeast-1-111122223333/textract-transformers/data/docs-train.manifest.jsonl\n",
      "upload: data/docs-val.manifest.jsonl to s3://sagemaker-ap-southeast-1-111122223333/textract-transformers/data/docs-val.manifest.jsonl\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp data/docs-train.manifest.jsonl {selfsup_train_manifest_s3uri}\n",
    "!aws s3 cp data/docs-val.manifest.jsonl {selfsup_val_manifest_s3uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Amazon Textract JSONs prepared on S3 and split between training and validation via manifests, we're ready to run the pre-training.\n",
    "\n",
    "> ▶️ See the following *Fine-tuning on annotated data* section for more details and links on how model training works in SageMaker - which are omitted here since this section is optional.\n",
    "\n",
    "Since customized inputs for this job might be more variable than fine-tuning (because annotating data requires effort, but scaling up your unlabelled corpus may be easy), it's worth mentioning some relevant parameter options:\n",
    "\n",
    "- **`instance_type`**: While `ml.g4dn.xlarge` is a nice, low-hourly-cost, GPU-enabled option for our small data fine-tuning job later; the larger data volume in pre-training makes the speed-up available from `ml.p3.2xlarge` more significant. The provided script is multi-GPU capable, so for bigger jobs you may find `ml.p3.8xlarge` and beyond give more acceptable run-times.\n",
    "- **`per_device_train_batch_size`**: Controls *per-accelerator* batching; so bear in mind that moving up to a multi-GPU instance type (such as 4 GPUs in an `ml.p3.8xlarge`) implicitly increases the overall batch size for learning.\n",
    "- Other hyperparameters are available, as the implementation is generally based on the [Hugging Face TrainingArguments parser](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments) with [customizations applied in src/code/config.py](src/code/config.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "hyperparameters = {\n",
    "    # (See src/code/config.py for more info on script parameters)\n",
    "    \"task_name\": \"mlm\",\n",
    "\n",
    "    # GT's augmented manifest are stored under s3://${bucket_name}/${bucket_prefix}/data/textracted/\n",
    "    # and textract_prefix is the above URI stripped of s3://${bucket_name}/${bucket_prefix}/.\n",
    "    \"textract_prefix\": textract_s3uri[len(\"s3://\") :].partition(\"/\")[2],\n",
    "\n",
    "    \"model_name_or_path\": \"microsoft/layoutlm-base-uncased\",\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"num_train_epochs\": 25,\n",
    "    \"early_stopping_patience\": 10,\n",
    "    \"metric_for_best_model\": \"eval_loss\",\n",
    "    \"greater_is_better\": \"false\",\n",
    "    # Early stopping implies checkpointing every evaluation (epoch), so limit the total checkpoints\n",
    "    # kept to avoid filling up disk:\n",
    "    \"save_total_limit\": 10,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "# fmt: on\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"epoch\", \"Regex\": util.training.get_hf_metric_regex(\"epoch\")},\n",
    "    {\"Name\": \"learning_rate\", \"Regex\": util.training.get_hf_metric_regex(\"learning_rate\")},\n",
    "    {\"Name\": \"train:loss\", \"Regex\": util.training.get_hf_metric_regex(\"loss\")},\n",
    "    {\"Name\": \"validation:loss\", \"Regex\": util.training.get_hf_metric_regex(\"eval_loss\")},\n",
    "    {\n",
    "        \"Name\": \"validation:samples_per_sec\",\n",
    "        \"Regex\": util.training.get_hf_metric_regex(\"eval_samples_per_second\"),\n",
    "    },\n",
    "]\n",
    "\n",
    "pre_estimator = HuggingFaceEstimator(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"src\",\n",
    "    py_version=\"py38\",\n",
    "    pytorch_version=\"1.9\",\n",
    "    transformers_version=\"4.11\",\n",
    "    base_job_name=\"layoutlm-cfpb-pretrain\",\n",
    "    output_path=f\"s3://{bucket_name}/{bucket_prefix}trainjobs\",\n",
    "    instance_type=\"ml.p3.8xlarge\",\n",
    "    instance_count=1,\n",
    "    volume_size=50,\n",
    "    debugger_hook_config=False,\n",
    "    profiler_config=sagemaker.debugger.ProfilerConfig(\n",
    "        framework_profile_params=sagemaker.debugger.FrameworkProfile(),\n",
    "    ),\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    "    # Required for our custom dataset loading code (which depends on tokenizer):\n",
    "    environment={\n",
    "        \"TOKENIZERS_PARALLELISM\": \"false\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-18 05:04:13 Starting - Starting the training job...\n",
      "2022-01-18 05:04:37 Starting - Launching requested ML instancesProfilerReport-1642482253: InProgress\n",
      "......\n",
      "2022-01-18 05:05:38 Starting - Preparing the instances for training.........\n",
      "2022-01-18 05:07:11 Downloading - Downloading input data...\n",
      "2022-01-18 05:07:38 Training - Downloading the training image........................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:31,692 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:31,731 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:31,739 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:32,334 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting amazon-textract-response-parser<0.2,>=0.1\n",
      "  Downloading amazon_textract_response_parser-0.1.24-py2.py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mCollecting marshmallow==3.11.1\n",
      "  Downloading marshmallow-3.11.1-py2.py3-none-any.whl (46 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.8/site-packages (from amazon-textract-response-parser<0.2,>=0.1->-r requirements.txt (line 1)) (1.18.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from boto3->amazon-textract-response-parser<0.2,>=0.1->-r requirements.txt (line 1)) (0.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3->amazon-textract-response-parser<0.2,>=0.1->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.22.0,>=1.21.30 in /opt/conda/lib/python3.8/site-packages (from boto3->amazon-textract-response-parser<0.2,>=0.1->-r requirements.txt (line 1)) (1.21.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore<1.22.0,>=1.21.30->boto3->amazon-textract-response-parser<0.2,>=0.1->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.22.0,>=1.21.30->boto3->amazon-textract-response-parser<0.2,>=0.1->-r requirements.txt (line 1)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.30->boto3->amazon-textract-response-parser<0.2,>=0.1->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: marshmallow, amazon-textract-response-parser\u001b[0m\n",
      "\u001b[34mSuccessfully installed amazon-textract-response-parser-0.1.24 marshmallow-3.11.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:35,601 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"textract\": \"/opt/ml/input/data/textract\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"task_name\": \"mlm\",\n",
      "        \"textract_prefix\": \"textract-transformers/data/textracted\",\n",
      "        \"seed\": 42,\n",
      "        \"metric_for_best_model\": \"eval_loss\",\n",
      "        \"early_stopping_patience\": 10,\n",
      "        \"save_total_limit\": 10,\n",
      "        \"num_train_epochs\": 25,\n",
      "        \"per_device_train_batch_size\": 4,\n",
      "        \"learning_rate\": 5e-05,\n",
      "        \"greater_is_better\": \"false\",\n",
      "        \"model_name_or_path\": \"microsoft/layoutlm-base-uncased\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"textract\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"layoutlm-cfpb-pretrain-2022-01-18-05-04-13-217\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-1-111122223333/layoutlm-cfpb-pretrain-2022-01-18-05-04-13-217/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"early_stopping_patience\":10,\"greater_is_better\":\"false\",\"learning_rate\":5e-05,\"metric_for_best_model\":\"eval_loss\",\"model_name_or_path\":\"microsoft/layoutlm-base-uncased\",\"num_train_epochs\":25,\"per_device_train_batch_size\":4,\"save_total_limit\":10,\"seed\":42,\"task_name\":\"mlm\",\"textract_prefix\":\"textract-transformers/data/textracted\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"textract\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"textract\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-southeast-1-111122223333/layoutlm-cfpb-pretrain-2022-01-18-05-04-13-217/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"textract\":\"/opt/ml/input/data/textract\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"early_stopping_patience\":10,\"greater_is_better\":\"false\",\"learning_rate\":5e-05,\"metric_for_best_model\":\"eval_loss\",\"model_name_or_path\":\"microsoft/layoutlm-base-uncased\",\"num_train_epochs\":25,\"per_device_train_batch_size\":4,\"save_total_limit\":10,\"seed\":42,\"task_name\":\"mlm\",\"textract_prefix\":\"textract-transformers/data/textracted\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"textract\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"layoutlm-cfpb-pretrain-2022-01-18-05-04-13-217\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-1-111122223333/layoutlm-cfpb-pretrain-2022-01-18-05-04-13-217/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--early_stopping_patience\",\"10\",\"--greater_is_better\",\"false\",\"--learning_rate\",\"5e-05\",\"--metric_for_best_model\",\"eval_loss\",\"--model_name_or_path\",\"microsoft/layoutlm-base-uncased\",\"--num_train_epochs\",\"25\",\"--per_device_train_batch_size\",\"4\",\"--save_total_limit\",\"10\",\"--seed\",\"42\",\"--task_name\",\"mlm\",\"--textract_prefix\",\"textract-transformers/data/textracted\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEXTRACT=/opt/ml/input/data/textract\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_TASK_NAME=mlm\u001b[0m\n",
      "\u001b[34mSM_HP_TEXTRACT_PREFIX=textract-transformers/data/textracted\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=42\u001b[0m\n",
      "\u001b[34mSM_HP_METRIC_FOR_BEST_MODEL=eval_loss\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_PATIENCE=10\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_TOTAL_LIMIT=10\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=25\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=5e-05\u001b[0m\n",
      "\u001b[34mSM_HP_GREATER_IS_BETTER=false\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=microsoft/layoutlm-base-uncased\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train.py --early_stopping_patience 10 --greater_is_better false --learning_rate 5e-05 --metric_for_best_model eval_loss --model_name_or_path microsoft/layoutlm-base-uncased --num_train_epochs 25 --per_device_train_batch_size 4 --save_total_limit 10 --seed 42 --task_name mlm --textract_prefix textract-transformers/data/textracted\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:42,208 [main] INFO Loaded arguments:\u001b[0m\n",
      "\u001b[34mModelArguments(cache_dir='/tmp/transformers/cache', config_name=None, model_name_or_path='microsoft/layoutlm-base-uncased', model_revision='main', tokenizer_name=None, use_auth_token=False)\u001b[0m\n",
      "\u001b[34mDataTrainingArguments(annotation_attr='labels', max_seq_length=512, max_train_samples=None, task_name='mlm', textract='/opt/ml/input/data/textract', textract_prefix='textract-transformers/data/textracted', train='/opt/ml/input/data/train', validation='/opt/ml/input/data/validation', num_labels=2, mlm_probability=0.15)\u001b[0m\n",
      "\u001b[34mSageMakerTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=4,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=30,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=True,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34mearly_stopping_patience=10,\u001b[0m\n",
      "\u001b[34mearly_stopping_threshold=0.0,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=IntervalStrategy.EPOCH,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgreater_is_better=False,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_strategy=HubStrategy.EVERY_SAVE,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=5e-05,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=True,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=-1,\u001b[0m\n",
      "\u001b[34mlog_level_replica=-1,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/tmp/transformers/checkpoints/runs/Jan18_05-11-41_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=SchedulerType.LINEAR,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=eval_loss,\u001b[0m\n",
      "\u001b[34mmodel_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=25.0,\u001b[0m\n",
      "\u001b[34moutput_dir=/tmp/transformers/checkpoints,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=True,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=8,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=4,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/tmp/transformers/checkpoints,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=IntervalStrategy.EPOCH,\u001b[0m\n",
      "\u001b[34msave_total_limit=10,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:42,208 [main] INFO Starting!\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:42,208 [main] INFO Creating config and model\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:43,159 [filelock] INFO Lock 139952581517120 acquired on /tmp/transformers/cache/c59c8df730b08b61677ea9f63ceb3885b726dc72bec3664931831934754e6255.a42e5a49c15f385384232820f72fe4a1f5a14244c183f75fbe7817241fe2ff50.lock\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1664] 2022-01-18 05:11:43,159 >> https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /tmp/transformers/cache/tmp38wm1etz\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1668] 2022-01-18 05:11:44,048 >> storing https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/config.json in cache at /tmp/transformers/cache/c59c8df730b08b61677ea9f63ceb3885b726dc72bec3664931831934754e6255.a42e5a49c15f385384232820f72fe4a1f5a14244c183f75fbe7817241fe2ff50\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1676] 2022-01-18 05:11:44,048 >> creating metadata file for /tmp/transformers/cache/c59c8df730b08b61677ea9f63ceb3885b726dc72bec3664931831934754e6255.a42e5a49c15f385384232820f72fe4a1f5a14244c183f75fbe7817241fe2ff50\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:44,048 [filelock] INFO Lock 139952581517120 released on /tmp/transformers/cache/c59c8df730b08b61677ea9f63ceb3885b726dc72bec3664931831934754e6255.a42e5a49c15f385384232820f72fe4a1f5a14244c183f75fbe7817241fe2ff50.lock\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:583] 2022-01-18 05:11:44,049 >> loading configuration file https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/config.json from cache at /tmp/transformers/cache/c59c8df730b08b61677ea9f63ceb3885b726dc72bec3664931831934754e6255.a42e5a49c15f385384232820f72fe4a1f5a14244c183f75fbe7817241fe2ff50\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:620] 2022-01-18 05:11:44,051 >> Model config LayoutLMConfig {\n",
      "  \"_name_or_path\": \"microsoft/layoutlm-base-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"mlm\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"0\",\n",
      "    \"1\": \"1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"0\": 0,\n",
      "    \"1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_2d_position_embeddings\": 1024,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"layoutlm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:45,048 [filelock] INFO Lock 139952581725344 acquired on /tmp/transformers/cache/ff1931049683ee1e934397a712f4202c59537de2fc0266e3587404cb18822f16.1a981b4b6ba73bb1d630760e2c7baf5bc300ce297d5bd57068fbaed633cc09f1.lock\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1664] 2022-01-18 05:11:45,048 >> https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /tmp/transformers/cache/tmp14ukmf54\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1668] 2022-01-18 05:11:45,979 >> storing https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/tokenizer_config.json in cache at /tmp/transformers/cache/ff1931049683ee1e934397a712f4202c59537de2fc0266e3587404cb18822f16.1a981b4b6ba73bb1d630760e2c7baf5bc300ce297d5bd57068fbaed633cc09f1\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1676] 2022-01-18 05:11:45,979 >> creating metadata file for /tmp/transformers/cache/ff1931049683ee1e934397a712f4202c59537de2fc0266e3587404cb18822f16.1a981b4b6ba73bb1d630760e2c7baf5bc300ce297d5bd57068fbaed633cc09f1\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:45,979 [filelock] INFO Lock 139952581725344 released on /tmp/transformers/cache/ff1931049683ee1e934397a712f4202c59537de2fc0266e3587404cb18822f16.1a981b4b6ba73bb1d630760e2c7baf5bc300ce297d5bd57068fbaed633cc09f1.lock\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:583] 2022-01-18 05:11:46,909 >> loading configuration file https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/config.json from cache at /tmp/transformers/cache/c59c8df730b08b61677ea9f63ceb3885b726dc72bec3664931831934754e6255.a42e5a49c15f385384232820f72fe4a1f5a14244c183f75fbe7817241fe2ff50\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:620] 2022-01-18 05:11:46,910 >> Model config LayoutLMConfig {\n",
      "  \"_name_or_path\": \"microsoft/layoutlm-base-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_2d_position_embeddings\": 1024,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"layoutlm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:48,716 [filelock] INFO Lock 139952581702992 acquired on /tmp/transformers/cache/960573f6d2723ce34fb09c7a76c98fa416881a36b0036eb98a9a9465a091319f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1664] 2022-01-18 05:11:48,716 >> https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /tmp/transformers/cache/tmpijm2nepl\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1668] 2022-01-18 05:11:50,406 >> storing https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/vocab.txt in cache at /tmp/transformers/cache/960573f6d2723ce34fb09c7a76c98fa416881a36b0036eb98a9a9465a091319f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1676] 2022-01-18 05:11:50,406 >> creating metadata file for /tmp/transformers/cache/960573f6d2723ce34fb09c7a76c98fa416881a36b0036eb98a9a9465a091319f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:50,406 [filelock] INFO Lock 139952581702992 released on /tmp/transformers/cache/960573f6d2723ce34fb09c7a76c98fa416881a36b0036eb98a9a9465a091319f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:51,300 [filelock] INFO Lock 139958797333696 acquired on /tmp/transformers/cache/78de759d8c688ac51b32f20d922ca1c1c3dbec5f9b3abbe9f3fcca22b815249f.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1664] 2022-01-18 05:11:51,301 >> https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /tmp/transformers/cache/tmpdjh7dvby\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1668] 2022-01-18 05:11:53,515 >> storing https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/tokenizer.json in cache at /tmp/transformers/cache/78de759d8c688ac51b32f20d922ca1c1c3dbec5f9b3abbe9f3fcca22b815249f.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1676] 2022-01-18 05:11:53,515 >> creating metadata file for /tmp/transformers/cache/78de759d8c688ac51b32f20d922ca1c1c3dbec5f9b3abbe9f3fcca22b815249f.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:53,515 [filelock] INFO Lock 139958797333696 released on /tmp/transformers/cache/78de759d8c688ac51b32f20d922ca1c1c3dbec5f9b3abbe9f3fcca22b815249f.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:55,349 [filelock] INFO Lock 139952581590176 acquired on /tmp/transformers/cache/48c3f426580c1b3278dbebb8c8dd372ea1549792f092b4f6fae1e21881c2cbd9.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d.lock\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1664] 2022-01-18 05:11:55,350 >> https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /tmp/transformers/cache/tmpcma6nr60\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1668] 2022-01-18 05:11:56,346 >> storing https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/special_tokens_map.json in cache at /tmp/transformers/cache/48c3f426580c1b3278dbebb8c8dd372ea1549792f092b4f6fae1e21881c2cbd9.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1676] 2022-01-18 05:11:56,346 >> creating metadata file for /tmp/transformers/cache/48c3f426580c1b3278dbebb8c8dd372ea1549792f092b4f6fae1e21881c2cbd9.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:56,346 [filelock] INFO Lock 139952581590176 released on /tmp/transformers/cache/48c3f426580c1b3278dbebb8c8dd372ea1549792f092b4f6fae1e21881c2cbd9.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d.lock\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-01-18 05:11:57,344 >> loading file https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/vocab.txt from cache at /tmp/transformers/cache/960573f6d2723ce34fb09c7a76c98fa416881a36b0036eb98a9a9465a091319f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-01-18 05:11:57,344 >> loading file https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/tokenizer.json from cache at /tmp/transformers/cache/78de759d8c688ac51b32f20d922ca1c1c3dbec5f9b3abbe9f3fcca22b815249f.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-01-18 05:11:57,344 >> loading file https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-01-18 05:11:57,344 >> loading file https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/special_tokens_map.json from cache at /tmp/transformers/cache/48c3f426580c1b3278dbebb8c8dd372ea1549792f092b4f6fae1e21881c2cbd9.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-01-18 05:11:57,344 >> loading file https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/tokenizer_config.json from cache at /tmp/transformers/cache/ff1931049683ee1e934397a712f4202c59537de2fc0266e3587404cb18822f16.1a981b4b6ba73bb1d630760e2c7baf5bc300ce297d5bd57068fbaed633cc09f1\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:58,297 [filelock] INFO Lock 139958797333696 acquired on /root/.cache/huggingface/transformers/c59c8df730b08b61677ea9f63ceb3885b726dc72bec3664931831934754e6255.a42e5a49c15f385384232820f72fe4a1f5a14244c183f75fbe7817241fe2ff50.lock\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1664] 2022-01-18 05:11:58,298 >> https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpleywcuhl\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1668] 2022-01-18 05:11:59,224 >> storing https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/c59c8df730b08b61677ea9f63ceb3885b726dc72bec3664931831934754e6255.a42e5a49c15f385384232820f72fe4a1f5a14244c183f75fbe7817241fe2ff50\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1676] 2022-01-18 05:11:59,224 >> creating metadata file for /root/.cache/huggingface/transformers/c59c8df730b08b61677ea9f63ceb3885b726dc72bec3664931831934754e6255.a42e5a49c15f385384232820f72fe4a1f5a14244c183f75fbe7817241fe2ff50\u001b[0m\n",
      "\u001b[34m2022-01-18 05:11:59,225 [filelock] INFO Lock 139958797333696 released on /root/.cache/huggingface/transformers/c59c8df730b08b61677ea9f63ceb3885b726dc72bec3664931831934754e6255.a42e5a49c15f385384232820f72fe4a1f5a14244c183f75fbe7817241fe2ff50.lock\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:583] 2022-01-18 05:11:59,225 >> loading configuration file https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c59c8df730b08b61677ea9f63ceb3885b726dc72bec3664931831934754e6255.a42e5a49c15f385384232820f72fe4a1f5a14244c183f75fbe7817241fe2ff50\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:620] 2022-01-18 05:11:59,225 >> Model config LayoutLMConfig {\n",
      "  \"_name_or_path\": \"microsoft/layoutlm-base-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_2d_position_embeddings\": 1024,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"layoutlm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2022-01-18 05:12:00,250 [filelock] INFO Lock 139952581738656 acquired on /tmp/transformers/cache/4a74c6c9128ba518e61fbdf559d03e64b6bd0ad6db588419dfd865ace535942a.a48b7b4437be34e24274c9cf6cf57e2424d3f1eec537ec03b905e6f01d19ed77.lock\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1664] 2022-01-18 05:12:00,250 >> https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/transformers/cache/tmp83zlk5br\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1668] 2022-01-18 05:12:07,038 >> storing https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/pytorch_model.bin in cache at /tmp/transformers/cache/4a74c6c9128ba518e61fbdf559d03e64b6bd0ad6db588419dfd865ace535942a.a48b7b4437be34e24274c9cf6cf57e2424d3f1eec537ec03b905e6f01d19ed77\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1676] 2022-01-18 05:12:07,038 >> creating metadata file for /tmp/transformers/cache/4a74c6c9128ba518e61fbdf559d03e64b6bd0ad6db588419dfd865ace535942a.a48b7b4437be34e24274c9cf6cf57e2424d3f1eec537ec03b905e6f01d19ed77\u001b[0m\n",
      "\u001b[34m2022-01-18 05:12:07,038 [filelock] INFO Lock 139952581738656 released on /tmp/transformers/cache/4a74c6c9128ba518e61fbdf559d03e64b6bd0ad6db588419dfd865ace535942a.a48b7b4437be34e24274c9cf6cf57e2424d3f1eec537ec03b905e6f01d19ed77.lock\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1323] 2022-01-18 05:12:07,038 >> loading weights file https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/pytorch_model.bin from cache at /tmp/transformers/cache/4a74c6c9128ba518e61fbdf559d03e64b6bd0ad6db588419dfd865ace535942a.a48b7b4437be34e24274c9cf6cf57e2424d3f1eec537ec03b905e6f01d19ed77\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1588] 2022-01-18 05:12:08,698 >> All model checkpoint weights were used when initializing LayoutLMForMaskedLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1596] 2022-01-18 05:12:08,699 >> All the weights of LayoutLMForMaskedLM were initialized from the model checkpoint at microsoft/layoutlm-base-uncased.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use LayoutLMForMaskedLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m2022-01-18 05:12:08,711 [main] INFO Loading datasets\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3298] 2022-01-18 05:12:08,913 >> Token indices sequence length is longer than the specified maximum sequence length for this model (924 > 512). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\n",
      "2022-01-18 05:12:39 Training - Training image download completed. Training in progress.\u001b[34m2022-01-18 05:12:52,134 [main] INFO train dataset has 2318 samples\u001b[0m\n",
      "\u001b[34m2022-01-18 05:12:52,135 [main] INFO validation dataset has 226 samples\u001b[0m\n",
      "\u001b[34m2022-01-18 05:12:52,135 [main] INFO Setting up trainer\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1196] 2022-01-18 05:12:56,735 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1197] 2022-01-18 05:12:56,736 >>   Num examples = 2318\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1198] 2022-01-18 05:12:56,736 >>   Num Epochs = 25\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1199] 2022-01-18 05:12:56,736 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1200] 2022-01-18 05:12:56,736 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1201] 2022-01-18 05:12:56,736 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1202] 2022-01-18 05:12:56,736 >>   Total optimization steps = 3625\u001b[0m\n",
      "\u001b[34malgo-1:28:28 [0] ofi_init:1134 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34mNCCL version 2.7.8+cuda11.1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:13:45,695 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:13:45,695 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:13:45,695 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.0799399614334106, 'eval_runtime': 3.029, 'eval_samples_per_second': 74.611, 'eval_steps_per_second': 2.641, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:13:48,724 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-145\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:13:48,726 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-145/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:13:49,460 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-145/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:13:49,460 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-145/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:13:49,460 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-145/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:14:31,710 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:14:31,710 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:14:31,710 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.6785320043563843, 'eval_runtime': 3.1211, 'eval_samples_per_second': 72.411, 'eval_steps_per_second': 2.563, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:14:34,831 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-290\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:14:34,832 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-290/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:14:35,550 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-290/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:14:35,550 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-290/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:14:35,550 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-290/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:15:17,270 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:15:17,270 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:15:17,270 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.5204957723617554, 'eval_runtime': 3.1249, 'eval_samples_per_second': 72.323, 'eval_steps_per_second': 2.56, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:15:20,396 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-435\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:15:20,397 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-435/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:15:21,135 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-435/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:15:21,136 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-435/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:15:21,136 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-435/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m{'loss': 1.2734, 'learning_rate': 4.3103448275862066e-05, 'epoch': 3.45}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:16:03,411 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:16:03,412 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:16:03,412 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.45156124234199524, 'eval_runtime': 3.0279, 'eval_samples_per_second': 74.64, 'eval_steps_per_second': 2.642, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:16:06,440 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-580\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:16:06,441 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-580/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:16:07,187 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-580/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:16:07,187 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-580/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:16:07,188 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-580/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:16:49,756 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:16:49,756 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:16:49,756 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.41694843769073486, 'eval_runtime': 3.1182, 'eval_samples_per_second': 72.478, 'eval_steps_per_second': 2.566, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:16:52,875 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-725\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:16:52,876 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-725/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:16:53,585 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-725/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:16:53,586 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-725/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:16:53,586 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-725/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:17:35,394 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:17:35,395 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:17:35,395 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3979779779911041, 'eval_runtime': 3.1286, 'eval_samples_per_second': 72.238, 'eval_steps_per_second': 2.557, 'epoch': 6.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:17:38,523 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-870\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:17:38,524 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-870/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:17:39,242 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-870/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:17:39,243 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-870/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:17:39,243 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-870/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m{'loss': 0.431, 'learning_rate': 3.620689655172414e-05, 'epoch': 6.9}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:18:21,184 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:18:21,185 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:18:21,185 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3815803825855255, 'eval_runtime': 3.0137, 'eval_samples_per_second': 74.992, 'eval_steps_per_second': 2.655, 'epoch': 7.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:18:24,198 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1015\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:18:24,200 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1015/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:18:24,917 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1015/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:18:24,917 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-1015/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:18:24,917 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-1015/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:19:06,988 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:19:06,988 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:19:06,988 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.33718422055244446, 'eval_runtime': 3.1425, 'eval_samples_per_second': 71.918, 'eval_steps_per_second': 2.546, 'epoch': 8.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:19:10,131 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1160\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:19:10,132 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1160/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:19:10,856 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1160/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:19:10,857 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-1160/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:19:10,857 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-1160/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:19:52,785 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:19:52,786 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:19:52,786 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.343288779258728, 'eval_runtime': 3.1696, 'eval_samples_per_second': 71.303, 'eval_steps_per_second': 2.524, 'epoch': 9.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:19:55,955 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1305\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:19:55,957 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1305/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:19:56,678 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1305/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:19:56,678 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-1305/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:19:56,678 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-1305/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:20:38,815 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:20:38,816 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:20:38,816 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.33965936303138733, 'eval_runtime': 3.1527, 'eval_samples_per_second': 71.684, 'eval_steps_per_second': 2.538, 'epoch': 10.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:20:41,968 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1450\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:20:41,969 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1450/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:20:42,686 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1450/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:20:42,687 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-1450/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:20:42,687 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-1450/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m{'loss': 0.3142, 'learning_rate': 2.9310344827586206e-05, 'epoch': 10.34}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:21:24,607 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:21:24,607 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:21:24,607 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.32127347588539124, 'eval_runtime': 3.1506, 'eval_samples_per_second': 71.731, 'eval_steps_per_second': 2.539, 'epoch': 11.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:21:27,758 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1595\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:21:27,759 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1595/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:21:28,471 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1595/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:21:28,471 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-1595/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:21:28,471 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-1595/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:21:29,928 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-145] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:22:10,700 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:22:10,700 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:22:10,700 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.321101576089859, 'eval_runtime': 3.1515, 'eval_samples_per_second': 71.713, 'eval_steps_per_second': 2.539, 'epoch': 12.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:22:13,852 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1740\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:22:13,853 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1740/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:22:14,594 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1740/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:22:14,595 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-1740/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:22:14,595 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-1740/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:22:16,079 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-290] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:22:56,840 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:22:56,840 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:22:56,840 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.305055171251297, 'eval_runtime': 3.0298, 'eval_samples_per_second': 74.592, 'eval_steps_per_second': 2.64, 'epoch': 13.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:22:59,870 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1885\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:22:59,871 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1885/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:23:00,601 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1885/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:23:00,602 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-1885/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:23:00,602 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-1885/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:23:02,066 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-435] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.2566, 'learning_rate': 2.2413793103448276e-05, 'epoch': 13.79}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:23:42,665 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:23:42,665 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:23:42,665 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2951352596282959, 'eval_runtime': 3.1486, 'eval_samples_per_second': 71.778, 'eval_steps_per_second': 2.541, 'epoch': 14.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:23:45,813 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2030\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:23:45,814 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2030/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:23:46,536 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2030/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:23:46,536 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-2030/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:23:46,536 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-2030/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:23:47,977 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-580] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:24:28,555 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:24:28,555 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:24:28,555 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.31483009457588196, 'eval_runtime': 3.17, 'eval_samples_per_second': 71.294, 'eval_steps_per_second': 2.524, 'epoch': 15.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:24:31,725 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2175\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:24:31,726 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2175/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:24:32,453 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2175/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:24:32,454 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-2175/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:24:32,454 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-2175/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:24:33,906 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-725] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:25:14,886 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:25:14,887 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:25:14,887 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2850310504436493, 'eval_runtime': 3.061, 'eval_samples_per_second': 73.831, 'eval_steps_per_second': 2.613, 'epoch': 16.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:25:17,948 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2320\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:25:17,949 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2320/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:25:18,670 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2320/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:25:18,670 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-2320/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:25:18,670 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-2320/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:25:20,116 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-870] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:26:00,817 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:26:00,817 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:26:00,818 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2914761006832123, 'eval_runtime': 3.1771, 'eval_samples_per_second': 71.135, 'eval_steps_per_second': 2.518, 'epoch': 17.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:26:03,994 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2465\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:26:04,002 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2465/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:26:04,715 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2465/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:26:04,715 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-2465/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:26:04,716 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-2465/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:26:06,174 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1015] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.2206, 'learning_rate': 1.5517241379310346e-05, 'epoch': 17.24}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:26:46,812 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:26:46,812 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:26:46,812 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.30409693717956543, 'eval_runtime': 3.1494, 'eval_samples_per_second': 71.76, 'eval_steps_per_second': 2.54, 'epoch': 18.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:26:49,961 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2610\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:26:49,963 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2610/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:26:50,681 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2610/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:26:50,682 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-2610/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:26:50,682 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-2610/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:26:52,133 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1160] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:27:32,752 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:27:32,752 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:27:32,752 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.281997948884964, 'eval_runtime': 3.1816, 'eval_samples_per_second': 71.033, 'eval_steps_per_second': 2.514, 'epoch': 19.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:27:35,934 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2755\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:27:35,935 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2755/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:27:36,648 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2755/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:27:36,649 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-2755/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:27:36,649 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-2755/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:27:38,098 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1305] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:28:18,863 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:28:18,864 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:28:18,864 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2744750380516052, 'eval_runtime': 3.1641, 'eval_samples_per_second': 71.427, 'eval_steps_per_second': 2.528, 'epoch': 20.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:28:22,028 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2900\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:28:22,029 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2900/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:28:22,756 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2900/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:28:22,757 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-2900/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:28:22,757 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-2900/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:28:24,201 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1450] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.1993, 'learning_rate': 8.620689655172414e-06, 'epoch': 20.69}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:29:05,207 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:29:05,208 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:29:05,208 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.27632030844688416, 'eval_runtime': 3.1024, 'eval_samples_per_second': 72.847, 'eval_steps_per_second': 2.579, 'epoch': 21.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:29:08,310 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3045\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:29:08,311 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3045/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:29:09,027 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3045/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:29:09,028 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-3045/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:29:09,028 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-3045/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:29:10,475 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1595] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:29:51,456 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:29:51,457 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:29:51,457 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2828299403190613, 'eval_runtime': 3.0499, 'eval_samples_per_second': 74.102, 'eval_steps_per_second': 2.623, 'epoch': 22.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:29:54,506 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3190\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:29:54,508 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3190/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:29:55,230 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3190/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:29:55,231 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-3190/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:29:55,231 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-3190/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:29:56,700 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1740] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:30:37,689 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:30:37,689 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:30:37,689 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.27919042110443115, 'eval_runtime': 3.1871, 'eval_samples_per_second': 70.912, 'eval_steps_per_second': 2.51, 'epoch': 23.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:30:40,877 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3335\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:30:40,878 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3335/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:30:41,607 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3335/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:30:41,608 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-3335/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:30:41,608 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-3335/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:30:43,059 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1885] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:31:23,879 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:31:23,879 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:31:23,879 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.28631678223609924, 'eval_runtime': 3.0875, 'eval_samples_per_second': 73.199, 'eval_steps_per_second': 2.591, 'epoch': 24.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:31:26,966 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3480\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:31:26,968 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3480/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:31:27,710 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3480/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:31:27,710 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-3480/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:31:27,710 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-3480/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:31:29,204 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2030] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.1844, 'learning_rate': 1.724137931034483e-06, 'epoch': 24.14}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:32:09,946 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:32:09,946 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:32:09,946 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2790319621562958, 'eval_runtime': 3.1723, 'eval_samples_per_second': 71.241, 'eval_steps_per_second': 2.522, 'epoch': 25.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:32:13,118 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3625\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:32:13,120 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3625/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:32:13,856 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3625/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:32:13,856 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-3625/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:32:13,857 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-3625/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:32:15,342 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2175] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1401] 2022-01-18 05:32:15,509 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1409] 2022-01-18 05:32:15,510 >> Loading best model from /tmp/transformers/checkpoints/checkpoint-2900 (score: 0.2744750380516052).\u001b[0m\n",
      "\u001b[34m{'train_runtime': 1159.1397, 'train_samples_per_second': 49.994, 'train_steps_per_second': 3.127, 'train_loss': 0.40345153335045125, 'epoch': 25.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:32:15,896 >> Saving model checkpoint to /tmp/transformers/checkpoints\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:32:15,897 >> Configuration saved in /tmp/transformers/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:32:16,616 >> Model weights saved in /tmp/transformers/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:32:16,617 >> tokenizer config file saved in /tmp/transformers/checkpoints/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:32:16,617 >> Special tokens file saved in /tmp/transformers/checkpoints/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m***** train metrics *****\n",
      "  epoch                    =       25.0\n",
      "  train_loss               =     0.4035\n",
      "  train_runtime            = 0:19:19.13\n",
      "  train_samples            =       2318\n",
      "  train_samples_per_second =     49.994\n",
      "  train_steps_per_second   =      3.127\u001b[0m\n",
      "\u001b[34m2022-01-18 05:32:16,663 [main] INFO Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:32:16,664 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:32:16,664 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:32:17,375 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:32:17,375 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:32:17,375 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m2022-01-18 05:32:17,421 [main] INFO Copying code to /opt/ml/model/code for inference\u001b[0m\n",
      "\u001b[34m2022-01-18 05:32:17,421 [main] INFO Copying ./requirements.txt to /opt/ml/model/code/requirements.txt\u001b[0m\n",
      "\u001b[34m2022-01-18 05:32:17,421 [main] INFO Copying ./train.py to /opt/ml/model/code/train.py\u001b[0m\n",
      "\u001b[34m2022-01-18 05:32:17,422 [main] INFO Copying ./__init__.py to /opt/ml/model/code/__init__.py\u001b[0m\n",
      "\u001b[34m2022-01-18 05:32:17,422 [main] INFO Copying ./inference.py to /opt/ml/model/code/inference.py\u001b[0m\n",
      "\u001b[34m2022-01-18 05:32:17,422 [main] INFO Copying ./code/config.py to /opt/ml/model/code/code/config.py\u001b[0m\n",
      "\u001b[34m2022-01-18 05:32:17,422 [main] INFO Copying ./code/train.py to /opt/ml/model/code/code/train.py\u001b[0m\n",
      "\u001b[34m2022-01-18 05:32:17,423 [main] INFO Copying ./code/__init__.py to /opt/ml/model/code/code/__init__.py\u001b[0m\n",
      "\u001b[34m2022-01-18 05:32:17,423 [main] INFO Copying ./code/logging_utils.py to /opt/ml/model/code/code/logging_utils.py\u001b[0m\n",
      "\u001b[34m2022-01-18 05:32:17,423 [main] INFO Copying ./code/inference.py to /opt/ml/model/code/code/inference.py\u001b[0m\n",
      "\u001b[34m2022-01-18 05:32:17,423 [main] INFO Copying ./code/data/mlm.py to /opt/ml/model/code/code/data/mlm.py\u001b[0m\n",
      "\u001b[34m2022-01-18 05:32:17,423 [main] INFO Copying ./code/data/base.py to /opt/ml/model/code/code/data/base.py\u001b[0m\n",
      "\u001b[34m2022-01-18 05:32:17,424 [main] INFO Copying ./code/data/ner.py to /opt/ml/model/code/code/data/ner.py\u001b[0m\n",
      "\u001b[34m2022-01-18 05:32:17,424 [main] INFO Copying ./code/data/__init__.py to /opt/ml/model/code/code/data/__init__.py\u001b[0m\n",
      "\u001b[34m2022-01-18 05:32:17,424 [main] INFO Copying ./code/data/geometry.py to /opt/ml/model/code/code/data/geometry.py\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1664] 2022-01-18 05:11:43,159 >> https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /tmp/transformers/cache/tmp38wm1etz\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/606 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 606/606 [00:00<00:00, 655kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1668] 2022-01-18 05:11:44,048 >> storing https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/config.json in cache at /tmp/transformers/cache/c59c8df730b08b61677ea9f63ceb3885b726dc72bec3664931831934754e6255.a42e5a49c15f385384232820f72fe4a1f5a14244c183f75fbe7817241fe2ff50\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1676] 2022-01-18 05:11:44,048 >> creating metadata file for /tmp/transformers/cache/c59c8df730b08b61677ea9f63ceb3885b726dc72bec3664931831934754e6255.a42e5a49c15f385384232820f72fe4a1f5a14244c183f75fbe7817241fe2ff50\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:583] 2022-01-18 05:11:44,049 >> loading configuration file https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/config.json from cache at /tmp/transformers/cache/c59c8df730b08b61677ea9f63ceb3885b726dc72bec3664931831934754e6255.a42e5a49c15f385384232820f72fe4a1f5a14244c183f75fbe7817241fe2ff50\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:620] 2022-01-18 05:11:44,051 >> Model config LayoutLMConfig {\n",
      "  \"_name_or_path\": \"microsoft/layoutlm-base-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"mlm\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"0\",\n",
      "    \"1\": \"1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"0\": 0,\n",
      "    \"1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_2d_position_embeddings\": 1024,\n",
      "  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m2022-01-18 05:32:19,093 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "  \"model_type\": \"layoutlm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1664] 2022-01-18 05:11:45,048 >> https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /tmp/transformers/cache/tmp14ukmf54\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/170 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 170/170 [00:00<00:00, 237kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1668] 2022-01-18 05:11:45,979 >> storing https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/tokenizer_config.json in cache at /tmp/transformers/cache/ff1931049683ee1e934397a712f4202c59537de2fc0266e3587404cb18822f16.1a981b4b6ba73bb1d630760e2c7baf5bc300ce297d5bd57068fbaed633cc09f1\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1676] 2022-01-18 05:11:45,979 >> creating metadata file for /tmp/transformers/cache/ff1931049683ee1e934397a712f4202c59537de2fc0266e3587404cb18822f16.1a981b4b6ba73bb1d630760e2c7baf5bc300ce297d5bd57068fbaed633cc09f1\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:583] 2022-01-18 05:11:46,909 >> loading configuration file https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/config.json from cache at /tmp/transformers/cache/c59c8df730b08b61677ea9f63ceb3885b726dc72bec3664931831934754e6255.a42e5a49c15f385384232820f72fe4a1f5a14244c183f75fbe7817241fe2ff50\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:620] 2022-01-18 05:11:46,910 >> Model config LayoutLMConfig {\n",
      "  \"_name_or_path\": \"microsoft/layoutlm-base-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_2d_position_embeddings\": 1024,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"layoutlm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1664] 2022-01-18 05:11:48,716 >> https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /tmp/transformers/cache/tmpijm2nepl\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]#015Downloading:   2%|▏         | 4.00k/226k [00:00<00:11, 19.7kB/s]#015Downloading:  12%|█▏        | 28.0k/226k [00:00<00:02, 77.6kB/s]#015Downloading:  38%|███▊      | 85.0k/226k [00:00<00:00, 170kB/s] #015Downloading:  87%|████████▋ | 197k/226k [00:00<00:00, 320kB/s] #015Downloading: 100%|██████████| 226k/226k [00:00<00:00, 278kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1668] 2022-01-18 05:11:50,406 >> storing https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/vocab.txt in cache at /tmp/transformers/cache/960573f6d2723ce34fb09c7a76c98fa416881a36b0036eb98a9a9465a091319f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1676] 2022-01-18 05:11:50,406 >> creating metadata file for /tmp/transformers/cache/960573f6d2723ce34fb09c7a76c98fa416881a36b0036eb98a9a9465a091319f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1664] 2022-01-18 05:11:51,301 >> https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /tmp/transformers/cache/tmpdjh7dvby\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]#015Downloading:   1%|          | 4.00k/455k [00:00<00:27, 16.9kB/s]#015Downloading:   8%|▊         | 36.0k/455k [00:00<00:04, 86.3kB/s]#015Downloading:  21%|██▏       | 97.0k/455k [00:00<00:02, 164kB/s] #015Downloading:  46%|████▌     | 209k/455k [00:00<00:00, 286kB/s] #015Downloading:  95%|█████████▌| 433k/455k [00:01<00:00, 523kB/s]#015Downloading: 100%|██████████| 455k/455k [00:01<00:00, 383kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1668] 2022-01-18 05:11:53,515 >> storing https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/tokenizer.json in cache at /tmp/transformers/cache/78de759d8c688ac51b32f20d922ca1c1c3dbec5f9b3abbe9f3fcca22b815249f.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1676] 2022-01-18 05:11:53,515 >> creating metadata file for /tmp/transformers/cache/78de759d8c688ac51b32f20d922ca1c1c3dbec5f9b3abbe9f3fcca22b815249f.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1664] 2022-01-18 05:11:55,350 >> https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /tmp/transformers/cache/tmpcma6nr60\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 112/112 [00:00<00:00, 164kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1668] 2022-01-18 05:11:56,346 >> storing https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/special_tokens_map.json in cache at /tmp/transformers/cache/48c3f426580c1b3278dbebb8c8dd372ea1549792f092b4f6fae1e21881c2cbd9.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1676] 2022-01-18 05:11:56,346 >> creating metadata file for /tmp/transformers/cache/48c3f426580c1b3278dbebb8c8dd372ea1549792f092b4f6fae1e21881c2cbd9.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-01-18 05:11:57,344 >> loading file https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/vocab.txt from cache at /tmp/transformers/cache/960573f6d2723ce34fb09c7a76c98fa416881a36b0036eb98a9a9465a091319f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-01-18 05:11:57,344 >> loading file https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/tokenizer.json from cache at /tmp/transformers/cache/78de759d8c688ac51b32f20d922ca1c1c3dbec5f9b3abbe9f3fcca22b815249f.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-01-18 05:11:57,344 >> loading file https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-01-18 05:11:57,344 >> loading file https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/special_tokens_map.json from cache at /tmp/transformers/cache/48c3f426580c1b3278dbebb8c8dd372ea1549792f092b4f6fae1e21881c2cbd9.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1741] 2022-01-18 05:11:57,344 >> loading file https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/tokenizer_config.json from cache at /tmp/transformers/cache/ff1931049683ee1e934397a712f4202c59537de2fc0266e3587404cb18822f16.1a981b4b6ba73bb1d630760e2c7baf5bc300ce297d5bd57068fbaed633cc09f1\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1664] 2022-01-18 05:11:58,298 >> https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpleywcuhl\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/606 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 606/606 [00:00<00:00, 917kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1668] 2022-01-18 05:11:59,224 >> storing https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/c59c8df730b08b61677ea9f63ceb3885b726dc72bec3664931831934754e6255.a42e5a49c15f385384232820f72fe4a1f5a14244c183f75fbe7817241fe2ff50\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1676] 2022-01-18 05:11:59,224 >> creating metadata file for /root/.cache/huggingface/transformers/c59c8df730b08b61677ea9f63ceb3885b726dc72bec3664931831934754e6255.a42e5a49c15f385384232820f72fe4a1f5a14244c183f75fbe7817241fe2ff50\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:583] 2022-01-18 05:11:59,225 >> loading configuration file https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c59c8df730b08b61677ea9f63ceb3885b726dc72bec3664931831934754e6255.a42e5a49c15f385384232820f72fe4a1f5a14244c183f75fbe7817241fe2ff50\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:620] 2022-01-18 05:11:59,225 >> Model config LayoutLMConfig {\n",
      "  \"_name_or_path\": \"microsoft/layoutlm-base-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_2d_position_embeddings\": 1024,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"layoutlm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1664] 2022-01-18 05:12:00,250 >> https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/transformers/cache/tmp83zlk5br\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/432M [00:00<?, ?B/s]#015Downloading:   1%|▏         | 5.55M/432M [00:00<00:07, 58.2MB/s]#015Downloading:   3%|▎         | 12.9M/432M [00:00<00:06, 69.2MB/s]#015Downloading:   5%|▍         | 20.4M/432M [00:00<00:05, 73.5MB/s]#015Downloading:   6%|▋         | 28.0M/432M [00:00<00:05, 75.9MB/s]#015Downloading:   8%|▊         | 35.6M/432M [00:00<00:05, 77.3MB/s]#015Downloading:  10%|▉         | 43.2M/432M [00:00<00:05, 78.1MB/s]#015Downloading:  12%|█▏        | 50.9M/432M [00:00<00:05, 78.9MB/s]#015Downloading:  14%|█▎        | 58.4M/432M [00:00<00:04, 79.1MB/s]#015Downloading:  15%|█▌        | 66.0M/432M [00:00<00:04, 79.1MB/s]#015Downloading:  17%|█▋        | 73.6M/432M [00:01<00:04, 79.4MB/s]#015Downloading:  19%|█▉        | 81.2M/432M [00:01<00:05, 61.6MB/s]#015Downloading:  20%|██        | 87.7M/432M [00:01<00:05, 62.2MB/s]#015Downloading:  22%|██▏       | 94.1M/432M [00:01<00:05, 63.7MB/s]#015Downloading:  23%|██▎       | 101M/432M [00:01<00:05, 64.9MB/s] #015Downloading:  25%|██▍       | 107M/432M [00:01<00:05, 65.9MB/s]#015Downloading:  26%|██▋       | 114M/432M [00:01<00:05, 66.5MB/s]#015Downloading:  28%|██▊       | 120M/432M [00:01<00:04, 66.5MB/s]#015Downloading:  29%|██▉       | 127M/432M [00:01<00:04, 67.2MB/s]#015Downloading:  31%|███       | 133M/432M [00:02<00:04, 67.7MB/s]#015Downloading:  32%|███▏      | 140M/432M [00:02<00:04, 68.1MB/s]#015Downloading:  34%|███▍      | 146M/432M [00:02<00:04, 68.1MB/s]#015Downloading:  35%|███▌      | 153M/432M [00:02<00:04, 67.9MB/s]#015Downloading:  37%|███▋      | 159M/432M [00:02<00:04, 67.6MB/s]#015Downloading:  38%|███▊      | 166M/432M [00:02<00:04, 67.7MB/s]#015Downloading:  40%|███▉      | 172M/432M [00:02<00:04, 67.5MB/s]#015Downloading:  41%|████▏     | 179M/432M [00:02<00:03, 67.7MB/s]#015Downloading:  43%|████▎     | 185M/432M [00:02<00:03, 67.8MB/s]#015Downloading:  44%|████▍     | 192M/432M [00:02<00:03, 68.0MB/s]#015Downloading:  46%|████▌     | 198M/432M [00:03<00:03, 68.4MB/s]#015Downloading:  47%|████▋     | 205M/432M [00:03<00:03, 68.5MB/s]#015Downloading:  49%|████▉     | 212M/432M [00:03<00:03, 68.5MB/s]#015Downloading:  50%|█████     | 218M/432M [00:03<00:03, 68.2MB/s]#015Downloading:  52%|█████▏    | 225M/432M [00:03<00:03, 68.1MB/s]#015Downloading:  53%|█████▎    | 231M/432M [00:03<00:03, 67.8MB/s]#015Downloading:  55%|█████▍    | 238M/432M [00:03<00:03, 67.4MB/s]#015Downloading:  56%|█████▋    | 244M/432M [00:03<00:02, 67.6MB/s]#015Downloading:  58%|█████▊    | 250M/432M [00:03<00:02, 67.4MB/s]#015Downloading:  59%|█████▉    | 257M/432M [00:03<00:02, 67.2MB/s]#015Downloading:  61%|██████    | 263M/432M [00:04<00:02, 67.3MB/s]#015Downloading:  62%|██████▏   | 270M/432M [00:04<00:02, 67.3MB/s]#015Downloading:  64%|██████▍   | 276M/432M [00:04<00:02, 67.4MB/s]#015Downloading:  65%|██████▌   | 283M/432M [00:04<00:02, 67.3MB/s]#015Downloading:  67%|██████▋   | 289M/432M [00:04<00:02, 67.3MB/s]#015Downloading:  68%|██████▊   | 296M/432M [00:04<00:02, 67.6MB/s]#015Downloading:  70%|██████▉   | 302M/432M [00:04<00:02, 67.6MB/s]#015Downloading:  71%|███████▏  | 308M/432M [00:04<00:02, 52.8MB/s]#015Downloading:  73%|███████▎  | 315M/432M [00:04<00:02, 56.2MB/s]#015Downloading:  74%|███████▍  | 321M/432M [00:05<00:01, 59.2MB/s]#015Downloading:  76%|███████▌  | 328M/432M [00:05<00:01, 62.5MB/s]#015Downloading:  78%|███████▊  | 335M/432M [00:05<00:01, 65.6MB/s]#015Downloading:  79%|███████▉  | 341M/432M [00:05<00:01, 65.7MB/s]#015Downloading:  81%|████████  | 348M/432M [00:05<00:01, 66.4MB/s]#015Downloading:  82%|████████▏ | 355M/432M [00:05<00:01, 67.1MB/s]#015Downloading:  84%|████████▎ | 361M/432M [00:05<00:01, 67.6MB/s]#015Downloading:  85%|████████▌ | 368M/432M [00:05<00:00, 67.8MB/s]#015Downloading:  87%|████████▋ | 374M/432M [00:05<00:00, 68.1MB/s]#015Downloading:  88%|████████▊ | 381M/432M [00:05<00:00, 58.9MB/s]#015Downloading:  90%|████████▉ | 387M/432M [00:06<00:00, 61.2MB/s]#015Downloading:  91%|█████████ | 394M/432M [00:06<00:00, 63.3MB/s]#015Downloading:  93%|█████████▎| 400M/432M [00:06<00:00, 64.7MB/s]#015Downloading:  94%|█████████▍| 407M/432M [00:06<00:00, 65.6MB/s]#015Downloading:  96%|█████████▌| 413M/432M [00:06<00:00, 66.5MB/s]#015Downloading:  97%|█████████▋| 420M/432M [00:06<00:00, 67.1MB/s]#015Downloading:  99%|█████████▊| 426M/432M [00:06<00:00, 67.6MB/s]#015Downloading: 100%|██████████| 432M/432M [00:06<00:00, 67.1MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1668] 2022-01-18 05:12:07,038 >> storing https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/pytorch_model.bin in cache at /tmp/transformers/cache/4a74c6c9128ba518e61fbdf559d03e64b6bd0ad6db588419dfd865ace535942a.a48b7b4437be34e24274c9cf6cf57e2424d3f1eec537ec03b905e6f01d19ed77\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1676] 2022-01-18 05:12:07,038 >> creating metadata file for /tmp/transformers/cache/4a74c6c9128ba518e61fbdf559d03e64b6bd0ad6db588419dfd865ace535942a.a48b7b4437be34e24274c9cf6cf57e2424d3f1eec537ec03b905e6f01d19ed77\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1323] 2022-01-18 05:12:07,038 >> loading weights file https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/pytorch_model.bin from cache at /tmp/transformers/cache/4a74c6c9128ba518e61fbdf559d03e64b6bd0ad6db588419dfd865ace535942a.a48b7b4437be34e24274c9cf6cf57e2424d3f1eec537ec03b905e6f01d19ed77\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1588] 2022-01-18 05:12:08,698 >> All model checkpoint weights were used when initializing LayoutLMForMaskedLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1596] 2022-01-18 05:12:08,699 >> All the weights of LayoutLMForMaskedLM were initialized from the model checkpoint at microsoft/layoutlm-base-uncased.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use LayoutLMForMaskedLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3298] 2022-01-18 05:12:08,913 >> Token indices sequence length is longer than the specified maximum sequence length for this model (924 > 512). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1196] 2022-01-18 05:12:56,735 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1197] 2022-01-18 05:12:56,736 >>   Num examples = 2318\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1198] 2022-01-18 05:12:56,736 >>   Num Epochs = 25\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1199] 2022-01-18 05:12:56,736 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1200] 2022-01-18 05:12:56,736 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1201] 2022-01-18 05:12:56,736 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1202] 2022-01-18 05:12:56,736 >>   Total optimization steps = 3625\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:13:45,695 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:13:45,695 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:13:45,695 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:13:48,724 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-145\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:13:48,726 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-145/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:13:49,460 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-145/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:13:49,460 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-145/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:13:49,460 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-145/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:14:31,710 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:14:31,710 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:14:31,710 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:14:34,831 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-290\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:14:34,832 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-290/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:14:35,550 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-290/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:14:35,550 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-290/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:14:35,550 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-290/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:15:17,270 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:15:17,270 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:15:17,270 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:15:20,396 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-435\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:15:20,397 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-435/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:15:21,135 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-435/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:15:21,136 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-435/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:15:21,136 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-435/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:16:03,411 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:16:03,412 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:16:03,412 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:16:06,440 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-580\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:16:06,441 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-580/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:16:07,187 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-580/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:16:07,187 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-580/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:16:07,188 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-580/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:16:49,756 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:16:49,756 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:16:49,756 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:16:52,875 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-725\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:16:52,876 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-725/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:16:53,585 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-725/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:16:53,586 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-725/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:16:53,586 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-725/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:17:35,394 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:17:35,395 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:17:35,395 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:17:38,523 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-870\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:17:38,524 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-870/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:17:39,242 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-870/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:17:39,243 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-870/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:17:39,243 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-870/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:18:21,184 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:18:21,185 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:18:21,185 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:18:24,198 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1015\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:18:24,200 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1015/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:18:24,917 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1015/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:18:24,917 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-1015/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:18:24,917 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-1015/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:19:06,988 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:19:06,988 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:19:06,988 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:19:10,131 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1160\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:19:10,132 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1160/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:19:10,856 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1160/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:19:10,857 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-1160/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:19:10,857 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-1160/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:19:52,785 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:19:52,786 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:19:52,786 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:19:55,955 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1305\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:19:55,957 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1305/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:19:56,678 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1305/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:19:56,678 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-1305/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:19:56,678 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-1305/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:20:38,815 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:20:38,816 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:20:38,816 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:20:41,968 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1450\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:20:41,969 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1450/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:20:42,686 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1450/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:20:42,687 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-1450/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:20:42,687 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-1450/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:21:24,607 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:21:24,607 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:21:24,607 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:21:27,758 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1595\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:21:27,759 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1595/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:21:28,471 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1595/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:21:28,471 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-1595/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:21:28,471 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-1595/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:21:29,928 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-145] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:22:10,700 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:22:10,700 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:22:10,700 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:22:13,852 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1740\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:22:13,853 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1740/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:22:14,594 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1740/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:22:14,595 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-1740/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:22:14,595 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-1740/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:22:16,079 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-290] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:22:56,840 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:22:56,840 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:22:56,840 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:22:59,870 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1885\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:22:59,871 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1885/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:23:00,601 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1885/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:23:00,602 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-1885/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:23:00,602 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-1885/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:23:02,066 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-435] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:23:42,665 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:23:42,665 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:23:42,665 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:23:45,813 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2030\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:23:45,814 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2030/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:23:46,536 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2030/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:23:46,536 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-2030/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:23:46,536 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-2030/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:23:47,977 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-580] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:24:28,555 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:24:28,555 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:24:28,555 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:24:31,725 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2175\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:24:31,726 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2175/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:24:32,453 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2175/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:24:32,454 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-2175/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:24:32,454 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-2175/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:24:33,906 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-725] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:25:14,886 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:25:14,887 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:25:14,887 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:25:17,948 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2320\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:25:17,949 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2320/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:25:18,670 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2320/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:25:18,670 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-2320/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:25:18,670 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-2320/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:25:20,116 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-870] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:26:00,817 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:26:00,817 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:26:00,818 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:26:03,994 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2465\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:26:04,002 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2465/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:26:04,715 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2465/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:26:04,715 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-2465/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:26:04,716 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-2465/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:26:06,174 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1015] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:26:46,812 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:26:46,812 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:26:46,812 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:26:49,961 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2610\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:26:49,963 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2610/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:26:50,681 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2610/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:26:50,682 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-2610/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:26:50,682 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-2610/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:26:52,133 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1160] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:27:32,752 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:27:32,752 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:27:32,752 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:27:35,934 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2755\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:27:35,935 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2755/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:27:36,648 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2755/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:27:36,649 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-2755/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:27:36,649 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-2755/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:27:38,098 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1305] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:28:18,863 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:28:18,864 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:28:18,864 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:28:22,028 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2900\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:28:22,029 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2900/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:28:22,756 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2900/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:28:22,757 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-2900/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:28:22,757 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-2900/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:28:24,201 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1450] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:29:05,207 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:29:05,208 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:29:05,208 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:29:08,310 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3045\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:29:08,311 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3045/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:29:09,027 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3045/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:29:09,028 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-3045/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:29:09,028 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-3045/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:29:10,475 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1595] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:29:51,456 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:29:51,457 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:29:51,457 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:29:54,506 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3190\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:29:54,508 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3190/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:29:55,230 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3190/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:29:55,231 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-3190/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:29:55,231 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-3190/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:29:56,700 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1740] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:30:37,689 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:30:37,689 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:30:37,689 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:30:40,877 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3335\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:30:40,878 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3335/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:30:41,607 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3335/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:30:41,608 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-3335/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:30:41,608 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-3335/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:30:43,059 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1885] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:31:23,879 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:31:23,879 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:31:23,879 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:31:26,966 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3480\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:31:26,968 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3480/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:31:27,710 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3480/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:31:27,710 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-3480/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:31:27,710 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-3480/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:31:29,204 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2030] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 05:32:09,946 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 05:32:09,946 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 05:32:09,946 >>   Batch size = 32\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:32:13,118 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3625\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:32:13,120 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3625/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:32:13,856 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3625/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:32:13,856 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-3625/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:32:13,857 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-3625/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 05:32:15,342 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2175] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1401] 2022-01-18 05:32:15,509 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1409] 2022-01-18 05:32:15,510 >> Loading best model from /tmp/transformers/checkpoints/checkpoint-2900 (score: 0.2744750380516052).\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:32:15,896 >> Saving model checkpoint to /tmp/transformers/checkpoints\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:32:15,897 >> Configuration saved in /tmp/transformers/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:32:16,616 >> Model weights saved in /tmp/transformers/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:32:16,617 >> tokenizer config file saved in /tmp/transformers/checkpoints/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:32:16,617 >> Special tokens file saved in /tmp/transformers/checkpoints/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 05:32:16,664 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 05:32:16,664 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 05:32:17,375 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 05:32:17,375 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 05:32:17,375 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\n",
      "2022-01-18 05:32:25 Uploading - Uploading generated training model\n",
      "2022-01-18 05:33:33 Completed - Training job completed\n",
      "Training seconds: 1582\n",
      "Billable seconds: 1582\n"
     ]
    }
   ],
   "source": [
    "# fmt: off\n",
    "if pretrain:\n",
    "    pre_estimator.fit(\n",
    "        inputs={\n",
    "            # URI: s3://.../data/docs-train.manifest.jsonl\n",
    "            # Content: each line defines s3://.../data/textracted/xxx/yyy.pdf/consolidated.json\n",
    "            \"train\": selfsup_train_manifest_s3uri,\n",
    "\n",
    "            # URI: s3://.../data/textracted/ which consists of xxx/yyy.pdf/consolidated.json\n",
    "            # Content: each json file is an augmented manifest files produced by GT\n",
    "            \"textract\": textract_s3uri + \"/\",\n",
    "\n",
    "            # Same as docs-train.manifest.jsonl\n",
    "            \"validation\": selfsup_val_manifest_s3uri,\n",
    "        },\n",
    "        # wait=False,\n",
    "    )\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the pre-training is complete, fetch the output model S3 URI to use as input for the fine-tuning stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom pre-trained model: s3:\u001b[35m/\u001b[0m\u001b[35m/sagemaker-ap-southeast-1-111122223333/textract-transformers/trainjobs/layoutlm-cfpb-pretrain-2022-01-18-05-04-13-217/output/\u001b[0m\u001b[95mmodel.tar.gz\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if pretrain:\n",
    "    # Un-comment this first line to load an previous pre-training job instead:\n",
    "    # pre_estimator = HuggingFaceEstimator.attach(\"layoutlm-cfpb-pretrain-2021-11-17-01-53-05-786\")\n",
    "\n",
    "    pretraining_job_desc = pre_estimator.latest_training_job.describe()\n",
    "    pretrained_s3_uri = pretraining_job_desc[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "print(f\"Custom pre-trained model: {pretrained_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning on annotated data\n",
    "\n",
    "In this section we'll run a [SageMaker Training Job](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html) to fine-tune the model on our annotated dataset.\n",
    "\n",
    "In this process:\n",
    "\n",
    "- SageMaker will run the job on a dedicated, managed instance of type we choose (we'll use `ml.p*` or `ml.g*` GPU-accelerated types), allowing us to keep this notebook's resources modest and only pay for the seconds of GPU time the training job needs.\n",
    "- The data as specified in the manifest files will be downloaded from Amazon S3.\n",
    "- The bundle of scripts we provide (in `src/`) will be transparently uploaded to S3 and then run inside the specified SageMaker-provided [framework container](https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers-prebuilt.html). There's no need for us to build our own container image or implement a serving stack for inference (although fully-custom containers are [also supported](https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers.html)).\n",
    "- Job hyperparameters will be passed through to our `src/` scripts as CLI arguments.\n",
    "- SageMaker will analyze the logs from the job (i.e. `print()` or `logger` calls from our script) with the regular expressions specified in `metric_definitions`, to scrape structured timeseries metrics like loss and accuracy.\n",
    "- When the job finishes, the contents of the `model` folder in the container will be automatically tarballed and uploaded to a `model.tar.gz` in Amazon S3.\n",
    "\n",
    "Rather than orchestrating this process through the low-level [SageMaker API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html) (e.g. via [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_training_job)), we'll use the open-source [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/) (`sagemaker`) for convenience.\n",
    "\n",
    "Rather than using the base [SageMaker PyTorch framework containers](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html), we'll take advantage of the [tailored containers for Hugging Face](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html). You can also refer to [Hugging Face's own docs for training on SageMaker](https://huggingface.co/transformers/sagemaker.html) for more information, and of course the implementation of our training script here in the `src/` folder.\n",
    "\n",
    "First, we'll configure some parameters you may **sometimes wish to re-use across training jobs**. Continuation jobs may want to use the same checkpoint location in S3, while from-scratch training should start fresh\n",
    "\n",
    "▶️ You can choose when to re-run this cell between experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoints to collection checkpoints-\u001b[1;36m2022\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m18\u001b[0m-\u001b[1;36m05\u001b[0m-\u001b[1;36m55\u001b[0m-\u001b[1;36m02\u001b[0m\n",
      "\u001b[32m's3://sagemaker-ap-southeast-1-111122223333/textract-transformers/checkpoints/checkpoints-2022-01-18-05-55-02'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "checkpoint_collection_name = \"checkpoints-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "print(f\"Saving checkpoints to collection {checkpoint_collection_name}\")\n",
    "\n",
    "checkpoint_s3_uri = f\"s3://{bucket_name}/{bucket_prefix}checkpoints/{checkpoint_collection_name}\"\n",
    "checkpoint_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the core configuration for our training job:\n",
    "\n",
    "▶️ This should usually be re-run for every new training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    # (See src/code/config.py for more info on script parameters)\n",
    "    \"annotation_attr\": standard_label_field,\n",
    "    \"textract_prefix\": textract_s3uri[len(\"s3://\") :].partition(\"/\")[2],\n",
    "    \"num_labels\": len(fields) + 1,  # +1 for \"other\"\n",
    "    \"num_train_epochs\": 150,  # Set high for automatic HP tuning later\n",
    "    \"early_stopping_patience\": 5,  # Usually stops after <20 epochs on this sample data+config\n",
    "    \"metric_for_best_model\": \"eval_focus_else_acc_minus_one\",\n",
    "    \"greater_is_better\": \"true\",\n",
    "    # Early stopping implies checkpointing every evaluation (epoch), so limit the total checkpoints\n",
    "    # kept to avoid filling up disk:\n",
    "    \"save_total_limit\": 10,\n",
    "}\n",
    "if not pretrained_s3_uri:\n",
    "    hyperparameters[\"model_name_or_path\"] = \"microsoft/layoutlm-base-uncased\"\n",
    "\n",
    "\n",
    "def get_hf_metric_regex(metric_name):\n",
    "    \"\"\"Build RegEx string to extract a numeric HuggingFace Transformers metric from logs\n",
    "\n",
    "    HF metric log lines look like a Python dict print e.g:\n",
    "    {'eval_loss': 0.3940396010875702, ..., 'epoch': 1.0}\n",
    "    \"\"\"\n",
    "    scientific_number_exp = r\"(-?[0-9]+(\\.[0-9]+)?(e[+\\-][0-9]+)?)\"\n",
    "    return \"\".join(\n",
    "        (\n",
    "            \"'\",\n",
    "            metric_name,\n",
    "            \"': \",\n",
    "            scientific_number_exp,\n",
    "            \"[,}]\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"epoch\", \"Regex\": get_hf_metric_regex(\"epoch\")},\n",
    "    {\"Name\": \"learning_rate\", \"Regex\": get_hf_metric_regex(\"learning_rate\")},\n",
    "    {\"Name\": \"train:loss\", \"Regex\": get_hf_metric_regex(\"loss\")},\n",
    "    {\"Name\": \"validation:n_examples\", \"Regex\": get_hf_metric_regex(\"eval_n_examples\")},\n",
    "    {\"Name\": \"validation:loss_avg\", \"Regex\": get_hf_metric_regex(\"eval_loss\")},\n",
    "    {\"Name\": \"validation:acc\", \"Regex\": get_hf_metric_regex(\"eval_acc\")},\n",
    "    {\"Name\": \"validation:focus_acc\", \"Regex\": get_hf_metric_regex(\"eval_focus_acc\")},\n",
    "    {\"Name\": \"validation:target\", \"Regex\": get_hf_metric_regex(\"eval_focus_else_acc_minus_one\")},\n",
    "]\n",
    "\n",
    "estimator = HuggingFaceEstimator(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"src\",\n",
    "    py_version=\"py38\",\n",
    "    pytorch_version=\"1.9\",\n",
    "    transformers_version=\"4.11\",\n",
    "    base_job_name=\"layoutlm-cfpb-hf\",\n",
    "    output_path=f\"s3://{bucket_name}/{bucket_prefix}trainjobs\",\n",
    "    # checkpoint_s3_uri=checkpoint_s3_uri,  # Un-comment to turn on checkpoint upload to S3\n",
    "    instance_type=\"ml.g4dn.xlarge\",  # Could also consider ml.p3.2xlarge\n",
    "    instance_count=1,\n",
    "    volume_size=50,\n",
    "    debugger_hook_config=False,\n",
    "    profiler_config=sagemaker.debugger.ProfilerConfig(\n",
    "        framework_profile_params=sagemaker.debugger.FrameworkProfile(),\n",
    "    ),\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    "    # Required for our custom dataset loading code (which depends on tokenizer):\n",
    "    environment={\n",
    "        \"TOKENIZERS_PARALLELISM\": \"false\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the below cell will actually kick off the training job and stream logs from the running container.\n",
    "\n",
    "> ℹ️ You'll also be able to check the status of the job in the [Training jobs page of the SageMaker Console](https://console.aws.amazon.com/sagemaker/home?#/jobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom pre-trained model s3:\u001b[35m/\u001b[0m\u001b[35m/sagemaker-ap-southeast-1-111122223333/textract-transformers/trainjobs/layoutlm-cfpb-pretrain-2022-01-18-05-04-13-217/output/\u001b[0m\u001b[95mmodel.tar.gz\u001b[0m\n",
      "2022-01-18 05:55:42 Starting - Starting the training job...\n",
      "2022-01-18 05:56:05 Starting - Launching requested ML instancesProfilerReport-1642485342: InProgress\n",
      "...\n",
      "2022-01-18 05:56:38 Starting - Preparing the instances for training.........\n",
      "2022-01-18 05:58:08 Downloading - Downloading input data\n",
      "2022-01-18 05:58:08 Training - Downloading the training image.........................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-01-18 06:02:12,918 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-01-18 06:02:12,936 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-01-18 06:02:12,942 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-01-18 06:02:13,380 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting amazon-textract-response-parser<0.2,>=0.1\n",
      "  Downloading amazon_textract_response_parser-0.1.24-py2.py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mCollecting marshmallow==3.11.1\n",
      "  Downloading marshmallow-3.11.1-py2.py3-none-any.whl (46 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.8/site-packages (from amazon-textract-response-parser<0.2,>=0.1->-r requirements.txt (line 1)) (1.18.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.22.0,>=1.21.30 in /opt/conda/lib/python3.8/site-packages (from boto3->amazon-textract-response-parser<0.2,>=0.1->-r requirements.txt (line 1)) (1.21.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from boto3->amazon-textract-response-parser<0.2,>=0.1->-r requirements.txt (line 1)) (0.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3->amazon-textract-response-parser<0.2,>=0.1->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore<1.22.0,>=1.21.30->boto3->amazon-textract-response-parser<0.2,>=0.1->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.22.0,>=1.21.30->boto3->amazon-textract-response-parser<0.2,>=0.1->-r requirements.txt (line 1)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.30->boto3->amazon-textract-response-parser<0.2,>=0.1->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: marshmallow, amazon-textract-response-parser\u001b[0m\n",
      "\u001b[34mSuccessfully installed amazon-textract-response-parser-0.1.24 marshmallow-3.11.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-01-18 06:02:15,716 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"textract\": \"/opt/ml/input/data/textract\",\n",
      "        \"model_name_or_path\": \"/opt/ml/input/data/model_name_or_path\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"textract_prefix\": \"textract-transformers/data/textracted\",\n",
      "        \"annotation_attr\": \"label\",\n",
      "        \"metric_for_best_model\": \"eval_focus_else_acc_minus_one\",\n",
      "        \"num_labels\": 20,\n",
      "        \"early_stopping_patience\": 5,\n",
      "        \"save_total_limit\": 10,\n",
      "        \"num_train_epochs\": 150,\n",
      "        \"greater_is_better\": \"true\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"textract\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"model_name_or_path\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"layoutlm-cfpb-hf-2022-01-18-05-55-42-136\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-1-111122223333/layoutlm-cfpb-hf-2022-01-18-05-55-42-136/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"annotation_attr\":\"label\",\"early_stopping_patience\":5,\"greater_is_better\":\"true\",\"metric_for_best_model\":\"eval_focus_else_acc_minus_one\",\"num_labels\":20,\"num_train_epochs\":150,\"save_total_limit\":10,\"textract_prefix\":\"textract-transformers/data/textracted\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model_name_or_path\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"textract\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model_name_or_path\",\"textract\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-southeast-1-111122223333/layoutlm-cfpb-hf-2022-01-18-05-55-42-136/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model_name_or_path\":\"/opt/ml/input/data/model_name_or_path\",\"textract\":\"/opt/ml/input/data/textract\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"annotation_attr\":\"label\",\"early_stopping_patience\":5,\"greater_is_better\":\"true\",\"metric_for_best_model\":\"eval_focus_else_acc_minus_one\",\"num_labels\":20,\"num_train_epochs\":150,\"save_total_limit\":10,\"textract_prefix\":\"textract-transformers/data/textracted\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model_name_or_path\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"textract\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"layoutlm-cfpb-hf-2022-01-18-05-55-42-136\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-1-111122223333/layoutlm-cfpb-hf-2022-01-18-05-55-42-136/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--annotation_attr\",\"label\",\"--early_stopping_patience\",\"5\",\"--greater_is_better\",\"true\",\"--metric_for_best_model\",\"eval_focus_else_acc_minus_one\",\"--num_labels\",\"20\",\"--num_train_epochs\",\"150\",\"--save_total_limit\",\"10\",\"--textract_prefix\",\"textract-transformers/data/textracted\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEXTRACT=/opt/ml/input/data/textract\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL_NAME_OR_PATH=/opt/ml/input/data/model_name_or_path\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_TEXTRACT_PREFIX=textract-transformers/data/textracted\u001b[0m\n",
      "\u001b[34mSM_HP_ANNOTATION_ATTR=label\u001b[0m\n",
      "\u001b[34mSM_HP_METRIC_FOR_BEST_MODEL=eval_focus_else_acc_minus_one\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LABELS=20\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_PATIENCE=5\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_TOTAL_LIMIT=10\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=150\u001b[0m\n",
      "\u001b[34mSM_HP_GREATER_IS_BETTER=true\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train.py --annotation_attr label --early_stopping_patience 5 --greater_is_better true --metric_for_best_model eval_focus_else_acc_minus_one --num_labels 20 --num_train_epochs 150 --save_total_limit 10 --textract_prefix textract-transformers/data/textracted\u001b[0m\n",
      "\u001b[34mGot pretrained model folder with contents: ['model.tar.gz']\u001b[0m\n",
      "\u001b[34mExtracting model tarball model.tar.gz to /opt/ml/input/data/model_name_or_path\u001b[0m\n",
      "\u001b[34mModel folder top-level contents: ['config.json', 'pytorch_model.bin', 'vocab.txt', 'model.tar.gz', 'tokenizer_config.json', 'code', 'special_tokens_map.json', 'tokenizer.json', 'training_args.bin']\u001b[0m\n",
      "\u001b[34m2022-01-18 06:02:23,541 [main] INFO Loaded arguments:\u001b[0m\n",
      "\u001b[34mModelArguments(cache_dir='/tmp/transformers/cache', config_name=None, model_name_or_path='/opt/ml/input/data/model_name_or_path', model_revision='main', tokenizer_name=None, use_auth_token=False)\u001b[0m\n",
      "\u001b[34mDataTrainingArguments(annotation_attr='label', max_seq_length=512, max_train_samples=None, task_name='ner', textract='/opt/ml/input/data/textract', textract_prefix='textract-transformers/data/textracted', train='/opt/ml/input/data/train', validation='/opt/ml/input/data/validation', num_labels=20, mlm_probability=0.15)\u001b[0m\n",
      "\u001b[34mSageMakerTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=2,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=True,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34mearly_stopping_patience=5,\u001b[0m\n",
      "\u001b[34mearly_stopping_threshold=0.0,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=IntervalStrategy.EPOCH,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgreater_is_better=True,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_strategy=HubStrategy.EVERY_SAVE,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=5e-05,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=True,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=-1,\u001b[0m\n",
      "\u001b[34mlog_level_replica=-1,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/tmp/transformers/checkpoints/runs/Jan18_06-02-23_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=SchedulerType.LINEAR,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=eval_focus_else_acc_minus_one,\u001b[0m\n",
      "\u001b[34mmodel_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=150.0,\u001b[0m\n",
      "\u001b[34moutput_dir=/tmp/transformers/checkpoints,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=True,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=8,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=4,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/tmp/transformers/checkpoints,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=IntervalStrategy.EPOCH,\u001b[0m\n",
      "\u001b[34msave_total_limit=10,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m2022-01-18 06:02:23,541 [main] INFO Starting!\u001b[0m\n",
      "\u001b[34m2022-01-18 06:02:23,541 [main] INFO Creating config and model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:581] 2022-01-18 06:02:23,542 >> loading configuration file /opt/ml/input/data/model_name_or_path/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:620] 2022-01-18 06:02:23,543 >> Model config LayoutLMConfig {\n",
      "  \"_name_or_path\": \"microsoft/layoutlm-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"LayoutLMForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"ner\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"0\",\n",
      "    \"1\": \"1\",\n",
      "    \"2\": \"2\",\n",
      "    \"3\": \"3\",\n",
      "    \"4\": \"4\",\n",
      "    \"5\": \"5\",\n",
      "    \"6\": \"6\",\n",
      "    \"7\": \"7\",\n",
      "    \"8\": \"8\",\n",
      "    \"9\": \"9\",\n",
      "    \"10\": \"10\",\n",
      "    \"11\": \"11\",\n",
      "    \"12\": \"12\",\n",
      "    \"13\": \"13\",\n",
      "    \"14\": \"14\",\n",
      "    \"15\": \"15\",\n",
      "    \"16\": \"16\",\n",
      "    \"17\": \"17\",\n",
      "    \"18\": \"18\",\n",
      "    \"19\": \"19\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"0\": 0,\n",
      "    \"1\": 1,\n",
      "    \"10\": 10,\n",
      "    \"11\": 11,\n",
      "    \"12\": 12,\n",
      "    \"13\": 13,\n",
      "    \"14\": 14,\n",
      "    \"15\": 15,\n",
      "    \"16\": 16,\n",
      "    \"17\": 17,\n",
      "    \"18\": 18,\n",
      "    \"19\": 19,\n",
      "    \"2\": 2,\n",
      "    \"3\": 3,\n",
      "    \"4\": 4,\n",
      "    \"5\": 5,\n",
      "    \"6\": 6,\n",
      "    \"7\": 7,\n",
      "    \"8\": 8,\n",
      "    \"9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_2d_position_embeddings\": 1024,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"layoutlm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1671] 2022-01-18 06:02:23,545 >> Didn't find file /opt/ml/input/data/model_name_or_path/added_tokens.json. We won't load it.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-18 06:02:23,545 >> loading file /opt/ml/input/data/model_name_or_path/vocab.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-18 06:02:23,545 >> loading file /opt/ml/input/data/model_name_or_path/tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-18 06:02:23,545 >> loading file None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-18 06:02:23,545 >> loading file /opt/ml/input/data/model_name_or_path/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-18 06:02:23,546 >> loading file /opt/ml/input/data/model_name_or_path/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1321] 2022-01-18 06:02:23,599 >> loading weights file /opt/ml/input/data/model_name_or_path/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1579] 2022-01-18 06:02:24,775 >> Some weights of the model checkpoint at /opt/ml/input/data/model_name_or_path were not used when initializing LayoutLMForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing LayoutLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing LayoutLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1590] 2022-01-18 06:02:24,775 >> Some weights of LayoutLMForTokenClassification were not initialized from the model checkpoint at /opt/ml/input/data/model_name_or_path and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m2022-01-18 06:02:24,777 [main] INFO Loading datasets\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3298] 2022-01-18 06:02:25,077 >> Token indices sequence length is longer than the specified maximum sequence length for this model (604 > 512). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m2022-01-18 06:02:50,451 [main] INFO train dataset has 209 samples\u001b[0m\n",
      "\u001b[34m2022-01-18 06:02:50,451 [main] INFO validation dataset has 21 samples\u001b[0m\n",
      "\u001b[34m2022-01-18 06:02:50,451 [main] INFO Setting up trainer\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1196] 2022-01-18 06:02:53,929 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1197] 2022-01-18 06:02:53,929 >>   Num examples = 209\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1198] 2022-01-18 06:02:53,929 >>   Num Epochs = 150\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1199] 2022-01-18 06:02:53,929 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1200] 2022-01-18 06:02:53,929 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1201] 2022-01-18 06:02:53,929 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1202] 2022-01-18 06:02:53,929 >>   Total optimization steps = 7950\u001b[0m\n",
      "\n",
      "2022-01-18 06:03:10 Training - Training image download completed. Training in progress.\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:03:16,181 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:03:16,181 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:03:16,181 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m2022-01-18 06:03:16,930 [data.ner] INFO Evaluation class prediction ratios: {4: 0.002190923317683881, 13: 0.012832550860719875, 14: 0.0014606155451225874, 15: 0.0004173187271778821, 16: 0.0009389671361502347, 17: 0.006364110589462702, 19: 0.9757955138236828}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.4310784935951233, 'eval_n_examples': 21, 'eval_acc': 0.9091914009621562, 'eval_focus_acc': 0.06620767293810122, 'eval_focus_else_acc_minus_one': 0.06620767293810122, 'eval_runtime': 0.7491, 'eval_samples_per_second': 28.033, 'eval_steps_per_second': 4.005, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:03:16,931 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-53\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:03:16,932 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-53/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:03:17,596 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-53/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:03:17,597 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-53/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:03:17,597 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-53/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:03:39,962 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:03:39,962 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:03:39,962 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m2022-01-18 06:03:40,718 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0020865936358894104, 4: 0.0026082420448617634, 12: 0.004277516953573292, 13: 0.022222222222222223, 14: 0.003755868544600939, 15: 0.003755868544600939, 16: 0.010119979134063642, 17: 0.00020865936358894105, 18: 0.0007303077725612937, 19: 0.9502347417840376}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2824941873550415, 'eval_n_examples': 21, 'eval_acc': 0.9350506567626469, 'eval_focus_acc': 0.3740039899352996, 'eval_focus_else_acc_minus_one': 0.3740039899352996, 'eval_runtime': 0.7568, 'eval_samples_per_second': 27.748, 'eval_steps_per_second': 3.964, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:03:40,719 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-106\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:03:40,720 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-106/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:03:41,385 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-106/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:03:41,385 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-106/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:03:41,385 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-106/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:04:03,768 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:04:03,768 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:04:03,768 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m2022-01-18 06:04:04,509 [data.ner] INFO Evaluation class prediction ratios: {0: 0.003129890453834116, 1: 0.0018779342723004694, 4: 0.008346374543557642, 10: 0.00198226395409494, 11: 0.0012519561815336462, 12: 0.004486176317162233, 13: 0.02170057381324987, 14: 0.006885758998435055, 15: 0.0038601982263954094, 16: 0.010641627543035994, 17: 0.0008346374543557642, 18: 0.012623891497130934, 19: 0.9223787167449139}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.17985691130161285, 'eval_n_examples': 21, 'eval_acc': 0.9500571298238769, 'eval_focus_acc': 0.46709645528256116, 'eval_focus_else_acc_minus_one': 0.46709645528256116, 'eval_runtime': 0.7422, 'eval_samples_per_second': 28.294, 'eval_steps_per_second': 4.042, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:04:04,510 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-159\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:04:04,511 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-159/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:04:05,174 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-159/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:04:05,175 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-159/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:04:05,175 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-159/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:04:27,680 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:04:27,680 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:04:27,680 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m2022-01-18 06:04:28,448 [data.ner] INFO Evaluation class prediction ratios: {0: 0.003755868544600939, 1: 0.003129890453834116, 4: 0.009076682316118936, 10: 0.001564945226917058, 11: 0.0014606155451225874, 12: 0.004486176317162233, 13: 0.020552947313510695, 14: 0.010328638497652582, 15: 0.004486176317162233, 16: 0.010954616588419406, 18: 0.026708398539384455, 19: 0.9034950443401147}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.13033811748027802, 'eval_n_examples': 21, 'eval_acc': 0.9607737068620119, 'eval_focus_acc': 0.5423481877469597, 'eval_focus_else_acc_minus_one': 0.5423481877469597, 'eval_runtime': 0.7695, 'eval_samples_per_second': 27.292, 'eval_steps_per_second': 3.899, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:04:28,450 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-212\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:04:28,451 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-212/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:04:29,112 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-212/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:04:29,113 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-212/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:04:29,113 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-212/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:04:51,744 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:04:51,744 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:04:51,744 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m2022-01-18 06:04:52,533 [data.ner] INFO Evaluation class prediction ratios: {0: 0.003547209181011998, 1: 0.0020865936358894104, 2: 0.00020865936358894105, 4: 0.006259780907668232, 10: 0.002921231090245175, 11: 0.001356285863328117, 12: 0.004486176317162233, 13: 0.02034428794992175, 14: 0.006885758998435055, 15: 0.004486176317162233, 16: 0.011267605633802818, 17: 0.00031298904538341156, 18: 0.012310902451747522, 19: 0.9235263432446531}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.22129744291305542, 'eval_n_examples': 21, 'eval_acc': 0.9494039356933166, 'eval_focus_acc': 0.48040844239026165, 'eval_focus_else_acc_minus_one': 0.48040844239026165, 'eval_runtime': 0.7902, 'eval_samples_per_second': 26.576, 'eval_steps_per_second': 3.797, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:04:52,534 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-265\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:04:52,535 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-265/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:04:53,202 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-265/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:04:53,203 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-265/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:04:53,203 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-265/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:05:15,950 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:05:15,950 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:05:15,950 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m2022-01-18 06:05:16,731 [data.ner] INFO Evaluation class prediction ratios: {0: 0.002190923317683881, 1: 0.003338549817423057, 2: 0.0025039123630672924, 3: 0.003547209181011998, 4: 0.003129890453834116, 8: 0.00020865936358894105, 10: 0.0022952529994783514, 11: 0.0008346374543557642, 12: 0.004486176317162233, 13: 0.019300991131977047, 14: 0.007407407407407408, 15: 0.0030255607720396454, 16: 0.010954616588419406, 18: 0.01189358372456964, 19: 0.9248826291079812}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2262161374092102, 'eval_n_examples': 21, 'eval_acc': 0.9524010613383079, 'eval_focus_acc': 0.48504215968347725, 'eval_focus_else_acc_minus_one': 0.48504215968347725, 'eval_runtime': 0.7818, 'eval_samples_per_second': 26.86, 'eval_steps_per_second': 3.837, 'epoch': 6.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:05:16,732 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-318\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:05:16,733 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-318/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:05:17,393 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-318/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:05:17,393 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-318/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:05:17,394 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-318/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:05:40,239 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:05:40,240 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:05:40,240 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m2022-01-18 06:05:41,012 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0025039123630672924, 1: 0.0020865936358894104, 4: 0.010328638497652582, 8: 0.00031298904538341156, 10: 0.0020865936358894104, 11: 0.004694835680751174, 12: 0.004486176317162233, 13: 0.013354199269692228, 14: 0.008137715179968702, 15: 0.004486176317162233, 16: 0.011163275952008346, 18: 0.01554512258737611, 19: 0.9208137715179968}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.16773760318756104, 'eval_n_examples': 21, 'eval_acc': 0.9553120759737898, 'eval_focus_acc': 0.5102317144221965, 'eval_focus_else_acc_minus_one': 0.5102317144221965, 'eval_runtime': 0.773, 'eval_samples_per_second': 27.167, 'eval_steps_per_second': 3.881, 'epoch': 7.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:05:41,013 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-371\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:05:41,014 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-371/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:05:41,667 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-371/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:05:41,667 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-371/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:05:41,668 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-371/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:06:04,651 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:06:04,651 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:06:04,651 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m2022-01-18 06:06:05,431 [data.ner] INFO Evaluation class prediction ratios: {0: 0.002712571726656234, 1: 0.0032342201356285864, 2: 0.0004173187271778821, 4: 0.006677099634846114, 8: 0.00031298904538341156, 10: 0.0018779342723004694, 11: 0.001773604590505999, 12: 0.004486176317162233, 13: 0.016484089723526343, 14: 0.008659363588941054, 15: 0.004486176317162233, 16: 0.010745957224830464, 18: 0.011267605633802818, 19: 0.9268648930620762}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2216799110174179, 'eval_n_examples': 21, 'eval_acc': 0.9543729187173928, 'eval_focus_acc': 0.5014736313081066, 'eval_focus_else_acc_minus_one': 0.5014736313081066, 'eval_runtime': 0.7805, 'eval_samples_per_second': 26.907, 'eval_steps_per_second': 3.844, 'epoch': 8.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:06:05,432 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-424\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:06:05,434 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-424/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:06:06,294 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-424/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:06:06,295 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-424/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:06:06,295 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-424/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:06:29,416 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:06:29,417 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:06:29,417 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m2022-01-18 06:06:30,206 [data.ner] INFO Evaluation class prediction ratios: {0: 0.004173187271778821, 1: 0.003547209181011998, 3: 0.0007303077725612937, 4: 0.010954616588419406, 8: 0.00031298904538341156, 10: 0.0025039123630672924, 11: 0.0025039123630672924, 12: 0.007303077725612937, 13: 0.01575378195096505, 14: 0.008659363588941054, 15: 0.004486176317162233, 16: 0.011476264997391758, 18: 0.02587376108502869, 19: 0.9017214397496087}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.12900540232658386, 'eval_n_examples': 21, 'eval_acc': 0.9639410991526232, 'eval_focus_acc': 0.579659442443736, 'eval_focus_else_acc_minus_one': 0.579659442443736, 'eval_runtime': 0.7907, 'eval_samples_per_second': 26.559, 'eval_steps_per_second': 3.794, 'epoch': 9.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:06:30,207 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-477\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:06:30,208 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-477/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:06:30,922 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-477/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:06:30,923 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-477/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:06:30,923 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-477/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m{'loss': 0.1609, 'learning_rate': 4.685534591194969e-05, 'epoch': 9.43}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:06:54,086 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:06:54,086 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:06:54,086 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m2022-01-18 06:06:54,857 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0028169014084507044, 1: 0.003338549817423057, 3: 0.0005216484089723526, 4: 0.005425143453312468, 8: 0.00031298904538341156, 10: 0.002399582681272822, 11: 0.0020865936358894104, 12: 0.004486176317162233, 13: 0.018779342723004695, 14: 0.008972352634324466, 15: 0.004486176317162233, 16: 0.010850286906624936, 18: 0.016901408450704224, 19: 0.918622848200313}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.17819692194461823, 'eval_n_examples': 21, 'eval_acc': 0.9574191645285268, 'eval_focus_acc': 0.5087077980858359, 'eval_focus_else_acc_minus_one': 0.5087077980858359, 'eval_runtime': 0.7719, 'eval_samples_per_second': 27.205, 'eval_steps_per_second': 3.886, 'epoch': 10.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:06:54,858 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-530\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:06:54,859 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-530/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:06:55,591 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-530/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:06:55,592 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-530/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:06:55,592 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-530/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:07:18,480 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:07:18,480 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:07:18,480 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m2022-01-18 06:07:19,272 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0032342201356285864, 1: 0.0034428794992175274, 3: 0.0006259780907668231, 4: 0.007303077725612937, 8: 0.00020865936358894105, 10: 0.001773604590505999, 11: 0.0016692749087115284, 12: 0.004486176317162233, 13: 0.01554512258737611, 14: 0.008242044861763172, 15: 0.004486176317162233, 16: 0.010537297861241524, 18: 0.01575378195096505, 19: 0.9226917057902974}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2321300059556961, 'eval_n_examples': 21, 'eval_acc': 0.9585976545977593, 'eval_focus_acc': 0.5181726322225406, 'eval_focus_else_acc_minus_one': 0.5181726322225406, 'eval_runtime': 0.7925, 'eval_samples_per_second': 26.499, 'eval_steps_per_second': 3.786, 'epoch': 11.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:07:19,273 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-583\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:07:19,274 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-583/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:07:20,000 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-583/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:07:20,001 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-583/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:07:20,001 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-583/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 06:07:21,488 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-53] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:07:42,505 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:07:42,505 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:07:42,505 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m2022-01-18 06:07:43,269 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0030255607720396454, 1: 0.00396452790818988, 3: 0.0005216484089723526, 4: 0.007407407407407408, 6: 0.00010432968179447053, 8: 0.00031298904538341156, 10: 0.0022952529994783514, 11: 0.0026082420448617634, 12: 0.005529473135106938, 13: 0.016275430359937403, 14: 0.006781429316640584, 15: 0.004486176317162233, 16: 0.010641627543035994, 18: 0.018257694314032343, 19: 0.9177882107459572}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.19695225358009338, 'eval_n_examples': 21, 'eval_acc': 0.9624394869099488, 'eval_focus_acc': 0.5419270676000044, 'eval_focus_else_acc_minus_one': 0.5419270676000044, 'eval_runtime': 0.7648, 'eval_samples_per_second': 27.458, 'eval_steps_per_second': 3.923, 'epoch': 12.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:07:43,270 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-636\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:07:43,271 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-636/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:07:43,992 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-636/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:07:43,993 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-636/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:07:43,993 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-636/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 06:07:45,480 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-106] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:08:06,322 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:08:06,322 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:08:06,322 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m2022-01-18 06:08:07,088 [data.ner] INFO Evaluation class prediction ratios: {0: 0.002712571726656234, 1: 0.0034428794992175274, 3: 0.0004173187271778821, 4: 0.00970266040688576, 8: 0.00031298904538341156, 10: 0.0022952529994783514, 11: 0.0026082420448617634, 12: 0.004486176317162233, 13: 0.016797078768909755, 14: 0.008659363588941054, 15: 0.004486176317162233, 16: 0.010850286906624936, 18: 0.01961398017736046, 19: 0.9136150234741784}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.17708130180835724, 'eval_n_examples': 21, 'eval_acc': 0.964527131590947, 'eval_focus_acc': 0.5451138182741866, 'eval_focus_else_acc_minus_one': 0.5451138182741866, 'eval_runtime': 0.7669, 'eval_samples_per_second': 27.384, 'eval_steps_per_second': 3.912, 'epoch': 13.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:08:07,089 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-689\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:08:07,090 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-689/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:08:07,816 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-689/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:08:07,816 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-689/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:08:07,816 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-689/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 06:08:09,318 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-159] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:08:30,175 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:08:30,176 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:08:30,176 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m2022-01-18 06:08:30,927 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0034428794992175274, 1: 0.0034428794992175274, 2: 0.00031298904538341156, 3: 0.0004173187271778821, 4: 0.0099113197704747, 5: 0.0006259780907668231, 8: 0.00031298904538341156, 10: 0.002399582681272822, 11: 0.00198226395409494, 12: 0.004486176317162233, 13: 0.01784037558685446, 14: 0.006259780907668232, 15: 0.004486176317162233, 16: 0.0116849243609807, 18: 0.016588419405320815, 19: 0.9158059467918623}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.230777770280838, 'eval_n_examples': 21, 'eval_acc': 0.9617993811307829, 'eval_focus_acc': 0.5302246055644337, 'eval_focus_else_acc_minus_one': 0.5302246055644337, 'eval_runtime': 0.7518, 'eval_samples_per_second': 27.932, 'eval_steps_per_second': 3.99, 'epoch': 14.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:08:30,928 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-742\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:08:30,929 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-742/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:08:31,666 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-742/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:08:31,667 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-742/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:08:31,667 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-742/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 06:08:33,165 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-212] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1401] 2022-01-18 06:08:33,186 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1409] 2022-01-18 06:08:33,187 >> Loading best model from /tmp/transformers/checkpoints/checkpoint-477 (score: 0.579659442443736).\u001b[0m\n",
      "\u001b[34m{'train_runtime': 339.6875, 'train_samples_per_second': 92.291, 'train_steps_per_second': 23.404, 'train_loss': 0.11233062499938307, 'epoch': 14.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:08:33,617 >> Saving model checkpoint to /tmp/transformers/checkpoints\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:08:33,618 >> Configuration saved in /tmp/transformers/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:08:34,378 >> Model weights saved in /tmp/transformers/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:08:34,379 >> tokenizer config file saved in /tmp/transformers/checkpoints/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:08:34,379 >> Special tokens file saved in /tmp/transformers/checkpoints/special_tokens_map.json\u001b[0m\n",
      "\n",
      "2022-01-18 06:08:49 Uploading - Uploading generated training model\u001b[34m***** train metrics *****\n",
      "  epoch                    =       14.0\n",
      "  train_loss               =     0.1123\n",
      "  train_runtime            = 0:05:39.68\n",
      "  train_samples            =        209\n",
      "  train_samples_per_second =     92.291\n",
      "  train_steps_per_second   =     23.404\u001b[0m\n",
      "\u001b[34m2022-01-18 06:08:34,416 [main] INFO Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:08:34,416 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:08:34,417 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:08:35,174 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:08:35,203 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:08:35,203 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m2022-01-18 06:08:35,238 [main] INFO Copying code to /opt/ml/model/code for inference\u001b[0m\n",
      "\u001b[34m2022-01-18 06:08:35,239 [main] INFO Copying ./requirements.txt to /opt/ml/model/code/requirements.txt\u001b[0m\n",
      "\u001b[34m2022-01-18 06:08:35,240 [main] INFO Copying ./train.py to /opt/ml/model/code/train.py\u001b[0m\n",
      "\u001b[34m2022-01-18 06:08:35,240 [main] INFO Copying ./__init__.py to /opt/ml/model/code/__init__.py\u001b[0m\n",
      "\u001b[34m2022-01-18 06:08:35,241 [main] INFO Copying ./inference.py to /opt/ml/model/code/inference.py\u001b[0m\n",
      "\u001b[34m2022-01-18 06:08:35,241 [main] INFO Copying ./code/config.py to /opt/ml/model/code/code/config.py\u001b[0m\n",
      "\u001b[34m2022-01-18 06:08:35,242 [main] INFO Copying ./code/train.py to /opt/ml/model/code/code/train.py\u001b[0m\n",
      "\u001b[34m2022-01-18 06:08:35,242 [main] INFO Copying ./code/__init__.py to /opt/ml/model/code/code/__init__.py\u001b[0m\n",
      "\u001b[34m2022-01-18 06:08:35,243 [main] INFO Copying ./code/logging_utils.py to /opt/ml/model/code/code/logging_utils.py\u001b[0m\n",
      "\u001b[34m2022-01-18 06:08:35,244 [main] INFO Copying ./code/inference.py to /opt/ml/model/code/code/inference.py\u001b[0m\n",
      "\u001b[34m2022-01-18 06:08:35,244 [main] INFO Copying ./code/data/mlm.py to /opt/ml/model/code/code/data/mlm.py\u001b[0m\n",
      "\u001b[34m2022-01-18 06:08:35,245 [main] INFO Copying ./code/data/base.py to /opt/ml/model/code/code/data/base.py\u001b[0m\n",
      "\u001b[34m2022-01-18 06:08:35,246 [main] INFO Copying ./code/data/ner.py to /opt/ml/model/code/code/data/ner.py\u001b[0m\n",
      "\u001b[34m2022-01-18 06:08:35,247 [main] INFO Copying ./code/data/__init__.py to /opt/ml/model/code/code/data/__init__.py\u001b[0m\n",
      "\u001b[34m2022-01-18 06:08:35,247 [main] INFO Copying ./code/data/geometry.py to /opt/ml/model/code/code/data/geometry.py\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:581] 2022-01-18 06:02:23,542 >> loading configuration file /opt/ml/input/data/model_name_or_path/config.json\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:620] 2022-01-18 06:02:23,543 >> Model config LayoutLMConfig {\n",
      "  \"_name_or_path\": \"microsoft/layoutlm-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"LayoutLMForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"ner\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"0\",\n",
      "    \"1\": \"1\",\n",
      "    \"2\": \"2\",\n",
      "    \"3\": \"3\",\n",
      "    \"4\": \"4\",\n",
      "    \"5\": \"5\",\n",
      "    \"6\": \"6\",\n",
      "    \"7\": \"7\",\n",
      "    \"8\": \"8\",\n",
      "    \"9\": \"9\",\n",
      "    \"10\": \"10\",\n",
      "    \"11\": \"11\",\n",
      "    \"12\": \"12\",\n",
      "    \"13\": \"13\",\n",
      "    \"14\": \"14\",\n",
      "    \"15\": \"15\",\n",
      "    \"16\": \"16\",\n",
      "    \"17\": \"17\",\n",
      "    \"18\": \"18\",\n",
      "    \"19\": \"19\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"0\": 0,\n",
      "    \"1\": 1,\n",
      "    \"10\": 10,\n",
      "    \"11\": 11,\n",
      "    \"12\": 12,\n",
      "    \"13\": 13,\n",
      "    \"14\": 14,\n",
      "    \"15\": 15,\n",
      "    \"16\": 16,\u001b[0m\n",
      "\u001b[34m2022-01-18 06:08:36,080 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "    \"17\": 17,\n",
      "    \"18\": 18,\n",
      "    \"19\": 19,\n",
      "    \"2\": 2,\n",
      "    \"3\": 3,\n",
      "    \"4\": 4,\n",
      "    \"5\": 5,\n",
      "    \"6\": 6,\n",
      "    \"7\": 7,\n",
      "    \"8\": 8,\n",
      "    \"9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_2d_position_embeddings\": 1024,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"layoutlm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1671] 2022-01-18 06:02:23,545 >> Didn't find file /opt/ml/input/data/model_name_or_path/added_tokens.json. We won't load it.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-18 06:02:23,545 >> loading file /opt/ml/input/data/model_name_or_path/vocab.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-18 06:02:23,545 >> loading file /opt/ml/input/data/model_name_or_path/tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-18 06:02:23,545 >> loading file None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-18 06:02:23,545 >> loading file /opt/ml/input/data/model_name_or_path/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1739] 2022-01-18 06:02:23,546 >> loading file /opt/ml/input/data/model_name_or_path/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1321] 2022-01-18 06:02:23,599 >> loading weights file /opt/ml/input/data/model_name_or_path/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1579] 2022-01-18 06:02:24,775 >> Some weights of the model checkpoint at /opt/ml/input/data/model_name_or_path were not used when initializing LayoutLMForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing LayoutLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing LayoutLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1590] 2022-01-18 06:02:24,775 >> Some weights of LayoutLMForTokenClassification were not initialized from the model checkpoint at /opt/ml/input/data/model_name_or_path and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3298] 2022-01-18 06:02:25,077 >> Token indices sequence length is longer than the specified maximum sequence length for this model (604 > 512). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1196] 2022-01-18 06:02:53,929 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1197] 2022-01-18 06:02:53,929 >>   Num examples = 209\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1198] 2022-01-18 06:02:53,929 >>   Num Epochs = 150\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1199] 2022-01-18 06:02:53,929 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1200] 2022-01-18 06:02:53,929 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1201] 2022-01-18 06:02:53,929 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1202] 2022-01-18 06:02:53,929 >>   Total optimization steps = 7950\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:03:16,181 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:03:16,181 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:03:16,181 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:03:16,931 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-53\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:03:16,932 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-53/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:03:17,596 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-53/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:03:17,597 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-53/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:03:17,597 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-53/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:03:39,962 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:03:39,962 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:03:39,962 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:03:40,719 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-106\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:03:40,720 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-106/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:03:41,385 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-106/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:03:41,385 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-106/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:03:41,385 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-106/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:04:03,768 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:04:03,768 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:04:03,768 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:04:04,510 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-159\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:04:04,511 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-159/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:04:05,174 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-159/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:04:05,175 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-159/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:04:05,175 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-159/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:04:27,680 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:04:27,680 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:04:27,680 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:04:28,450 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-212\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:04:28,451 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-212/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:04:29,112 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-212/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:04:29,113 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-212/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:04:29,113 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-212/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:04:51,744 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:04:51,744 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:04:51,744 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:04:52,534 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-265\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:04:52,535 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-265/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:04:53,202 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-265/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:04:53,203 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-265/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:04:53,203 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-265/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:05:15,950 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:05:15,950 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:05:15,950 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:05:16,732 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-318\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:05:16,733 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-318/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:05:17,393 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-318/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:05:17,393 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-318/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:05:17,394 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-318/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:05:40,239 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:05:40,240 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:05:40,240 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:05:41,013 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-371\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:05:41,014 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-371/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:05:41,667 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-371/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:05:41,667 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-371/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:05:41,668 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-371/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:06:04,651 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:06:04,651 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:06:04,651 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:06:05,432 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-424\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:06:05,434 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-424/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:06:06,294 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-424/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:06:06,295 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-424/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:06:06,295 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-424/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:06:29,416 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:06:29,417 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:06:29,417 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:06:30,207 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-477\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:06:30,208 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-477/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:06:30,922 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-477/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:06:30,923 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-477/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:06:30,923 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-477/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:06:54,086 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:06:54,086 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:06:54,086 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:06:54,858 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-530\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:06:54,859 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-530/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:06:55,591 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-530/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:06:55,592 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-530/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:06:55,592 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-530/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:07:18,480 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:07:18,480 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:07:18,480 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:07:19,273 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-583\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:07:19,274 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-583/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:07:20,000 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-583/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:07:20,001 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-583/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:07:20,001 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-583/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 06:07:21,488 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-53] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:07:42,505 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:07:42,505 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:07:42,505 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:07:43,270 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-636\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:07:43,271 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-636/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:07:43,992 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-636/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:07:43,993 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-636/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:07:43,993 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-636/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 06:07:45,480 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-106] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:08:06,322 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:08:06,322 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:08:06,322 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:08:07,089 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-689\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:08:07,090 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-689/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:08:07,816 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-689/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:08:07,816 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-689/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:08:07,816 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-689/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 06:08:09,318 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-159] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2235] 2022-01-18 06:08:30,175 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2237] 2022-01-18 06:08:30,176 >>   Num examples = 21\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2240] 2022-01-18 06:08:30,176 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:08:30,928 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-742\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:08:30,929 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-742/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:08:31,666 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-742/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:08:31,667 >> tokenizer config file saved in /tmp/transformers/checkpoints/checkpoint-742/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:08:31,667 >> Special tokens file saved in /tmp/transformers/checkpoints/checkpoint-742/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2065] 2022-01-18 06:08:33,165 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-212] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1401] 2022-01-18 06:08:33,186 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1409] 2022-01-18 06:08:33,187 >> Loading best model from /tmp/transformers/checkpoints/checkpoint-477 (score: 0.579659442443736).\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:08:33,617 >> Saving model checkpoint to /tmp/transformers/checkpoints\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:08:33,618 >> Configuration saved in /tmp/transformers/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:08:34,378 >> Model weights saved in /tmp/transformers/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:08:34,379 >> tokenizer config file saved in /tmp/transformers/checkpoints/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:08:34,379 >> Special tokens file saved in /tmp/transformers/checkpoints/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1987] 2022-01-18 06:08:34,416 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:413] 2022-01-18 06:08:34,417 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1041] 2022-01-18 06:08:35,174 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2033] 2022-01-18 06:08:35,203 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2039] 2022-01-18 06:08:35,203 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\n",
      "2022-01-18 06:09:49 Completed - Training job completed\n",
      "ProfilerReport-1642485342: IssuesFound\n",
      "Training seconds: 702\n",
      "Billable seconds: 702\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"train\": train_manifest_s3uri,\n",
    "    \"textract\": textract_s3uri + \"/\",\n",
    "    \"validation\": test_manifest_s3uri,\n",
    "}\n",
    "if pretrained_s3_uri:\n",
    "    print(f\"Using custom pre-trained model {pretrained_s3_uri}\")\n",
    "    inputs[\"model_name_or_path\"] = pretrained_s3_uri\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Hyperparameter tuning\n",
    "\n",
    "Particularly when applying novel techniques or working in new domains, we'll often need to find good values for a range of different *hyperparameters* of our proposed algorithms.\n",
    "\n",
    "Rather than spending time manually adjusting these parameters, we can use [SageMaker Automatic Model Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html) which uses an intelligent [Bayesian optimization approach](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html) to efficiently and automatically search for high-performing combinations over several training jobs.\n",
    "\n",
    "You can optionally run the cell below to kick off an HPO job for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = sagemaker.tuner.HyperparameterTuner(\n",
    "    estimator,\n",
    "    \"validation:target\",\n",
    "    base_tuning_job_name=\"layoutlm-cfpb-hpo\",\n",
    "    hyperparameter_ranges={\n",
    "        \"learning_rate\": sagemaker.parameter.ContinuousParameter(\n",
    "            1e-8,\n",
    "            1e-3,\n",
    "            scaling_type=\"Logarithmic\",\n",
    "        ),\n",
    "        \"per_device_train_batch_size\": sagemaker.parameter.CategoricalParameter([2, 4, 8, 16]),\n",
    "        \"label_smoothing_factor\": sagemaker.parameter.CategoricalParameter([0.0, 1e-9, 1e-6, 1e-3]),\n",
    "    },\n",
    "    metric_definitions=metric_definitions,\n",
    "    strategy=\"Bayesian\",\n",
    "    objective_type=\"Maximize\",\n",
    "    max_jobs=21,\n",
    "    max_parallel_jobs=2,\n",
    "    # early_stopping_type=\"Auto\",  # Off by default - could consider turning it on\n",
    "    #     warm_start_config=sagemaker.tuner.WarmStartConfig(\n",
    "    #         warm_start_type=sagemaker.tuner.WarmStartTypes.IDENTICAL_DATA_AND_ALGORITHM,\n",
    "    #         parents={ \"layoutlm-cfpb-hpo-210723-1625\" },\n",
    "    #     ),\n",
    ")\n",
    "\n",
    "tuner.fit(\n",
    "    inputs={\n",
    "        \"train\": train_manifest_s3uri,\n",
    "        \"textract\": textract_s3uri + \"/\",\n",
    "        \"validation\": test_manifest_s3uri,\n",
    "    },\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This job will run asynchronously so won't block the notebook, but you can check on the status from the [Hyperparameter tuning jobs list](https://console.aws.amazon.com/sagemaker/home?#/hyper-tuning-jobs) of the SageMaker Console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "Once our model is trained (or maybe even automatically hyperparameter-tuned over several training jobs), it's ready to be deployed for real-time or batch inference.\n",
    "\n",
    "Note that if, for some reason, you need to recover the state of a previous training or tuning job after a notebook restart or similar, you can `attach()` to training or tuning jobs by name - as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, you can attach to a previous training job by name like this:\n",
    "# estimator = HuggingFaceEstimator.attach(\"layoutlm-cfpb-210529-0851-006-5ee95cde\")\n",
    "# tuner = sagemaker.tuner.HyperparameterTuner.attach(\"layoutlm-cfpb-hpo-210603-0542\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy one-click deployment\n",
    "\n",
    "For straightforward deployment, you can just call `estimator.deploy()` (or equivalently, `tuner.deploy()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = estimator.latest_training_job.describe()[\"TrainingJobName\"]\n",
    "# Or:\n",
    "# training_job_name = tuner.best_training_job()\n",
    "\n",
    "predictor = estimator.deploy(\n",
    "    # Avoid us accidentally deploying the model twice by setting name per training job:\n",
    "    endpoint_name=training_job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "    # TODO: Disable once debugging is done\n",
    "    env={\"PYTHONUNBUFFERED\": \"1\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Digging deeper into the model\n",
    "\n",
    "Alternatively, you may instead want to explore the artifacts saved by the training job, or edit the `code` script bundle before deploying the endpoint - especially for debugging any problems with inference. Let's see how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "smclient = boto3.client(\"sagemaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_desc = estimator.latest_training_job.describe()\n",
    "model_s3uri = training_job_desc[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "model_name = training_job_desc[\"TrainingJobName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-ap-southeast-1-111122223333/textract-transformers/trainjobs/layoutlm-cfpb-hf-2022-01-18-05-55-42-136/output/model.tar.gz to data/model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./data/model\n",
    "!aws s3 cp $model_s3uri ./data/model/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\n",
      "pytorch_model.bin\n",
      "vocab.txt\n",
      "tokenizer_config.json\n",
      "code/\n",
      "code/train.py\n",
      "code/inference.py\n",
      "code/__init__.py\n",
      "code/requirements.txt\n",
      "code/code/\n",
      "code/code/train.py\n",
      "code/code/data/\n",
      "code/code/data/ner.py\n",
      "code/code/data/mlm.py\n",
      "code/code/data/base.py\n",
      "code/code/data/__init__.py\n",
      "code/code/data/geometry.py\n",
      "code/code/logging_utils.py\n",
      "code/code/config.py\n",
      "code/code/inference.py\n",
      "code/code/__init__.py\n",
      "special_tokens_map.json\n",
      "tokenizer.json\n",
      "training_args.bin\n"
     ]
    }
   ],
   "source": [
    "!cd data/model && tar -xzvf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "try:\n",
    "    # Make sure we don't accidentally re-use same model:\n",
    "    smclient.delete_model(ModelName=model_name)\n",
    "    print(f\"Deleted existing model {model_name}\")\n",
    "except smclient.exceptions.ClientError as e:\n",
    "    if not (\n",
    "        e.response[\"Error\"][\"Code\"] in (404, \"404\")\n",
    "        or e.response[\"Error\"].get(\"Message\", \"\").startswith(\"Could not find model\")\n",
    "    ):\n",
    "        raise e\n",
    "\n",
    "model = HuggingFaceModel(\n",
    "    name=model_name,\n",
    "    model_data=model_s3uri,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    source_dir=\"src/\",\n",
    "    entry_point=\"inference.py\",\n",
    "    transformers_version=\"4.11\",\n",
    "    py_version=\"py38\",\n",
    "    pytorch_version=\"1.9\",\n",
    "    # TODO: Disable once debugging is done\n",
    "    env={\"PYTHONUNBUFFERED\": \"1\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Delete previous endpoint, if already in use:\n",
    "    predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "    print(\"Deleting previous endpoint...\")\n",
    "    time.sleep(8)\n",
    "except (NameError, smclient.exceptions.ResourceNotFound):\n",
    "    pass  # No existing endpoint to delete\n",
    "except smclient.exceptions.ClientError as e:\n",
    "    if \"Could not find\" not in e.response[\"Error\"].get(\"Message\", \"\"):\n",
    "        raise e\n",
    "\n",
    "\n",
    "print(\"Deploying model...\")\n",
    "predictor = model.deploy(\n",
    "    endpoint_name=training_job_desc[\"TrainingJobName\"],\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "    # TODO: Disable once debugging is done\n",
    "    env={\"PYTHONUNBUFFERED\": \"1\"},\n",
    ")\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model\n",
    "\n",
    "Once the deployment is complete, we're ready to try it out with some real-time requests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As with estimators, you can attach the notebook to a previously deployed endpoint like this:\n",
    "# from sagemaker.huggingface import HuggingFacePredictor\n",
    "# predictor = HuggingFacePredictor(\n",
    "#     \"layoutlm-cfpb-hf-2021-09-02-01-08-11-234\",\n",
    "#     serializer=sagemaker.serializers.JSONSerializer(),\n",
    "#     deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making requests and rendering results\n",
    "\n",
    "This model accepts Textract-like JSON (e.g. as returned by [AnalyzeDocument](https://docs.aws.amazon.com/textract/latest/dg/API_AnalyzeDocument.html#API_AnalyzeDocument_ResponseSyntax) or [DetectDocumentText](https://docs.aws.amazon.com/textract/latest/dg/API_DetectDocumentText.html#API_DetectDocumentText_ResponseSyntax) APIs) and classifies each `WORD` [block](https://docs.aws.amazon.com/textract/latest/dg/API_Block.html) according to the entity classes we defined earlier: Returning the same JSON with additional fields added to indicate the predictions.\n",
    "\n",
    "We can use utility functions to render these predictions as we did the manual annotations previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import trp\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "\n",
    "def predict_from_manifest_item(\n",
    "    item,\n",
    "    predictor,\n",
    "    imgs_s3key_prefix=imgs_s3uri[len(\"s3://\") :].partition(\"/\")[2],\n",
    "    textract_s3key_prefix=textract_s3uri[len(\"s3://\") :].partition(\"/\")[2],\n",
    "    imgs_local_prefix=\"data/imgs-clean\",\n",
    "    textract_local_prefix=\"data/textracted\",\n",
    "    draw=True,\n",
    "):\n",
    "    paths = util.viz.local_paths_from_manifest_item(\n",
    "        item,\n",
    "        imgs_s3key_prefix,\n",
    "        textract_s3key_prefix=textract_s3key_prefix,\n",
    "        imgs_local_prefix=imgs_local_prefix,\n",
    "        textract_local_prefix=textract_local_prefix,\n",
    "    )\n",
    "\n",
    "    ## Basic inline request/response may fail for large, multi-page documents (because of breaking\n",
    "    ## the 5MB payload limit; or the model running out of memory):\n",
    "    #     with open(paths[\"textract\"], \"r\") as ftextract:\n",
    "    #         result_json = predictor.predict(json.loads(ftextract.read()))\n",
    "\n",
    "    ## We can strip the JSON down to only the target page of interest like this:\n",
    "    #     with open(paths[\"textract\"], \"r\") as ftextract:\n",
    "    #         result_json = predictor.predict({\n",
    "    #             \"Blocks\": trp.Document(\n",
    "    #                 json.loads(ftextract.read()),\n",
    "    #             ).pages[item[\"page-num\"] - 1].blocks\n",
    "    #         })\n",
    "\n",
    "    ## Or have the model refer directly to S3 and return us only the page of interest:\n",
    "    result_json = predictor.predict(\n",
    "        {\n",
    "            \"S3Input\": {\"URI\": item[\"textract-ref\"]},\n",
    "            \"TargetPageNum\": item[\"page-num\"],\n",
    "            \"TargetPageOnly\": True,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ## If we wanted, we could even have the model save results to S3 and fetch them ourselves:\n",
    "    #     result_json = predictor.predict({\n",
    "    #         \"S3Input\": { \"URI\": item[\"textract-ref\"] },\n",
    "    #         \"TargetPageNum\": item[\"page-num\"],\n",
    "    #         \"TargetPageOnly\": True,\n",
    "    #         \"S3Output\": { \"Bucket\": bucket_name, \"Key\": f\"{bucket_prefix}tmp/model-result\" },\n",
    "    #     })\n",
    "    #     result_json = json.loads(s3.Bucket(result_json[\"Bucket\"]).Object(result_json[\"Key\"]).get()[\"Body\"].read())\n",
    "\n",
    "    result_trp = trp.Document(result_json)\n",
    "\n",
    "    if draw:\n",
    "        util.viz.draw_smgt_annotated_page(\n",
    "            paths[\"image\"],\n",
    "            entity_classes,\n",
    "            annotations=[],\n",
    "            textract_result=result_trp,\n",
    "            # Note that page_num should be item[\"page-num\"] if we requested the full set of pages\n",
    "            # from the model above:\n",
    "            page_num=1,\n",
    "        )\n",
    "    return result_trp\n",
    "\n",
    "\n",
    "widgets.interact(\n",
    "    lambda ix: predict_from_manifest_item(\n",
    "        test_examples[ix],\n",
    "        predictor,\n",
    "    ),\n",
    "    ix=widgets.IntSlider(\n",
    "        min=0,\n",
    "        max=len(test_examples) - 1,\n",
    "        step=1,\n",
    "        value=0,\n",
    "        description=\"Example:\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From token classification to entity detection\n",
    "\n",
    "You may have noticed a slight mismatch: We're talking about extracting 'fields' or 'entities' from the document, but our model just classifies individual words. Going from words to entities assumes we're able to understand which words go \"together\" and what order they should be read in.\n",
    "\n",
    "Fortunately, Textract helps us out with this too as the word blocks are already collected into `LINE`s.\n",
    "\n",
    "For many straightforward applications, we can simply loop through the lines on a page and define an \"entity detection\" as a contiguous group of the same class - as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = predict_from_manifest_item(\n",
    "    test_examples[6],\n",
    "    predictor,\n",
    "    draw=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_cls = len(entity_classes)\n",
    "prev_cls = other_cls\n",
    "current_entity = \"\"\n",
    "\n",
    "for page in res.pages:\n",
    "    for line in page.lines:\n",
    "        for word in line.words:\n",
    "            pred_cls = word._block[\"PredictedClass\"]\n",
    "            if pred_cls != prev_cls:\n",
    "                if prev_cls != other_cls:\n",
    "                    print(f\"----------\\n{entity_classes[prev_cls]}:\\n{current_entity}\")\n",
    "                prev_cls = pred_cls\n",
    "                if pred_cls != other_cls:\n",
    "                    current_entity = word.text\n",
    "                else:\n",
    "                    current_entity = \"\"\n",
    "                continue\n",
    "            current_entity = \" \".join((current_entity, word.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course there may be some instances where this heuristic breaks down, but we still have access to all the position (and text) information from each `LINE` and `WORD` to write additional rules for reading order and separation if wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating the model with the OCR Pipeline\n",
    "\n",
    "If you've deployed the **OCR pipeline stack** in your AWS Account, you can now configure it to use this endpoint as follows:\n",
    "\n",
    "- First, identify the **endpoint name** of your deployed model. Assuming you created the predictor as above, you can simply run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, identify the **AWS Systems Manager Parameter** that configures the SageMaker endpoint for the OCR pipeline stack.\n",
    "\n",
    "The below code should pull it through for you, but alternatively you can refer to your stack's **Outputs** in the [AWS CloudFormation Console](https://console.aws.amazon.com/cloudformation/home?#/stacks). The Output name should include `SageMakerEndpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.sagemaker_endpoint_name_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, we'll update this SSM parameter to point to the deployed SageMaker endpoint.\n",
    "\n",
    "The below code should do this for you automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Configuring pipeline with model: {predictor.endpoint_name}\")\n",
    "\n",
    "ssm = boto3.client(\"ssm\")\n",
    "ssm.put_parameter(\n",
    "    Name=config.sagemaker_endpoint_name_param,\n",
    "    Overwrite=True,\n",
    "    Value=predictor.endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you could open the [AWS Systems Manager Parameter Store console](https://console.aws.amazon.com/systems-manager/parameters/?tab=Table) and click on the *name* of the parameter to open its detail page, then the **Edit** button in the top right corner as shown below:\n",
    "\n",
    "![](img/ssm-param-detail-screenshot.png \"Screenshot of SSM parameter detail page showing Edit button\")\n",
    "\n",
    "From this screen you can manually set the **Value** of the parameter and save the changes.\n",
    "\n",
    "Whether you updated the SSM parameter via code or the console, your stack is now configured to use the deployed model for OCR enrichment!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the pipeline entity definitions\n",
    "\n",
    "As well as configuring the *enrichment* stage of the pipeline to reference the deployed version of the model, we need to configure the *post-processing* stage to match the model's **definition of entity/field types**.\n",
    "\n",
    "The entity configuration is as we saved in the previous notebook, but the `annotation_guidance` attributes are not needed:\n",
    "\n",
    "> ℹ️ **Note:** As well as the mapping from ID numbers (returned by the model) to human-readable class names, this configuration controls how the pipeline consolidates entity matches into \"fields\" of the document: E.g. choosing the \"most likely\" or \"first\" value between multiple detections, or setting up a multi-value field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_entity_config = json.dumps(\n",
    "    [f.to_dict(omit=[\"annotation_guidance\"]) for f in fields], indent=2\n",
    ")\n",
    "print(pipeline_entity_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, you *could* set this value manually in the SSM console for the parameter named as `EntityConfig`.\n",
    "\n",
    "...But we can make the same update via code through the APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Setting pipeline entity configuration\")\n",
    "ssm.put_parameter(\n",
    "    Name=config.entity_config_param,\n",
    "    Overwrite=True,\n",
    "    Value=pipeline_entity_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out the pipeline\n",
    "\n",
    "To see the pipeline in action:\n",
    "\n",
    "▶️ **Open** the [AWS Step Functions Console](https://console.aws.amazon.com/states/home?#/statemachines) and click on the name of your *State Machine* from the list to see its details.\n",
    "\n",
    "(If you can't find it in the list, the code below should look it up for you or you can check the *Outputs* tab of your pipeline stack in the [AWS CloudFormation Console](https://console.aws.amazon.com/cloudformation/home?#/stacks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Your pipeline state machine is:\")\n",
    "print(config.pipeline_sfn_arn.rpartition(\":\")[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ **Locate** your pipeline's `InputBucket` in [Amazon S3](https://s3.console.aws.amazon.com/s3/home?)\n",
    "\n",
    "(Likewise you can look this up from CloudFormation or using the below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Your pipeline's input S3 bucket:\")\n",
    "print(config.pipeline_input_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ **Upload** a sample document (PDF) from our dataset to the S3 bucket\n",
    "\n",
    "You can do this by dragging and dropping the file to the S3 console - or running the cells below to upload a test document through the AWS CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfpaths = []\n",
    "for currpath, dirs, files in os.walk(\"data/raw\"):\n",
    "    if \"/.\" in currpath or \"__\" in currpath:\n",
    "        continue\n",
    "    pdfpaths += [os.path.join(currpath, f) for f in files if f.lower().endswith(\".pdf\")]\n",
    "pdfpaths = sorted(pdfpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filepath = pdfpaths[0]\n",
    "test_s3uri = f\"s3://{config.pipeline_input_bucket_name}/{test_filepath}\"\n",
    "\n",
    "!aws s3 cp '{test_filepath}' '{test_s3uri}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that a new *execution* (run) of the state machine is triggered automatically:\n",
    "\n",
    "> ℹ️ This may take a few seconds after the upload is complete. If you're not seeing it:\n",
    ">\n",
    "> - Check you're in the correct \"pipeline\" state machine, as this solution's stack creates more than one state machine\n",
    "> - Try refreshing the page or the execution list\n",
    "\n",
    "![](img/sfn-statemachine-screenshot.png \"Screenshot of AWS Step Functions state machine detail page showing execution list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clicking through to the execution, you'll be able to see the progress through the workflow and output/error information.\n",
    "\n",
    "Depending on your configuration, your view may look a little different to the below and you may have **either a successful execution or a failure at the review step**:\n",
    "\n",
    "![](img/sfn-execution-status-screenshot.png \"Screenshot of Step Functions execution detail view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "You should now have been able to train and deploy the enrichment model, and demonstrate its integration to the pipeline.\n",
    "\n",
    "However, the final human review stage is not fully set up yet, so may have triggered an error.\n",
    "\n",
    "In the final notebook, we'll configure the human review functionality to finish up the flow: Open up **notebook [3. Human Review.ipynb](3.%20Human%20Review.ipynb)** to follow along.\n",
    "\n",
    "\n",
    "### A note on clean-up\n",
    "\n",
    "Note that while training, processing and transform jobs in SageMaker start and stop compute resources for the specific job being executed, deployed **endpoints** stay active (and therefore accumulating charges) until you turn them off.\n",
    "\n",
    "When you're finished using an endpoint, you should delete it either through the [Amazon SageMaker Console](https://console.aws.amazon.com/sagemaker/home?#/endpoints) or via commands like the below.\n",
    "\n",
    "(Of course, our OCR pipeline stack will throw an error if you try to run it configured with an Endpoint Name that no longer exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Environment (virtualenv_layoutlm-p39)",
   "language": "python",
   "name": "virtualenv_layoutlm-p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
