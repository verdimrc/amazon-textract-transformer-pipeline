{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae1eb62-854f-423a-84f8-6c3d7eb5cfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m{\u001b[0m\n",
      "    \u001b[32m'data_prefix'\u001b[0m: \u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data'\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[32m'imgs'\u001b[0m: \u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/imgs-clean'\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[32m'textracted'\u001b[0m: \u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/textracted'\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[32m'annotations'\u001b[0m: \u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/annotations'\u001b[0m\u001b[1m)\u001b[0m\n",
      "\u001b[1m}\u001b[0m\n",
      "\n",
      "\u001b[1m{\u001b[0m\n",
      "    \u001b[32m'data_prefix'\u001b[0m: \u001b[1;35mS3Path\u001b[0m\u001b[1m(\u001b[0m\u001b[32m's3://sagemaker-ap-southeast-1-111122223333/textract-transformers/data'\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[32m'imgs'\u001b[0m: \u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/imgs-clean'\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[32m'textracted'\u001b[0m: \u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/textracted'\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[32m'annotations'\u001b[0m: \u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/annotations'\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[32m'bucket_name'\u001b[0m: \u001b[1;35mS3Path\u001b[0m\u001b[1m(\u001b[0m\u001b[32m's3://sagemaker-ap-southeast-1-111122223333'\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[32m'bucket_prefix'\u001b[0m: \u001b[32m'textract-transformers'\u001b[0m\n",
      "\u001b[1m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "%config IPythonBackend.figure_format = 'retina'\n",
    "\n",
    "import os\n",
    "from dataclasses import asdict\n",
    "\n",
    "from project_config import DataPaths, S3DataPaths\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from my_code.data import mlm\n",
    "from my_code.train import get_model\n",
    "\n",
    "try:\n",
    "    import rich\n",
    "\n",
    "    rich.reconfigure(force_terminal=True, force_jupyter=False)\n",
    "    rich.pretty.install()\n",
    "    print = rich.get_console().out\n",
    "    rprint = rich.get_console().print\n",
    "except:\n",
    "    pass\n",
    "\n",
    "datapaths_local = DataPaths()\n",
    "datapaths_s3 = S3DataPaths()\n",
    "display(asdict(datapaths_local), asdict(datapaths_s3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e78d8dc-a279-4aff-825b-e2521ff0a6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mModelArguments\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcache_dir\u001b[0m=\u001b[32m'/tmp/transformers/cache'\u001b[0m, \u001b[33mconfig_name\u001b[0m=\u001b[3;35mNone\u001b[0m, \n",
      "\u001b[33mmodel_name_or_path\u001b[0m=\u001b[32m'microsoft/layoutlm-base-uncased'\u001b[0m, \u001b[33mmodel_revision\u001b[0m=\u001b[32m'main'\u001b[0m, \n",
      "\u001b[33mtokenizer_name\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33muse_auth_token\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
      "\n",
      "\u001b[1;35mDataTrainingArguments\u001b[0m\u001b[1m(\u001b[0m\u001b[33mannotation_attr\u001b[0m=\u001b[32m'labels'\u001b[0m, \u001b[33mmax_seq_length\u001b[0m=\u001b[1;36m512\u001b[0m, \n",
      "\u001b[33mmax_train_samples\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33mtask_name\u001b[0m=\u001b[32m'mlm'\u001b[0m, \u001b[33mtextract\u001b[0m=\u001b[32m'/opt/ml/input/data/textract'\u001b[0m,\n",
      "\u001b[33mtextract_prefix\u001b[0m=\u001b[32m'textract-transformers/data/textracted'\u001b[0m, \n",
      "\u001b[33mtrain\u001b[0m=\u001b[32m'/opt/ml/input/data/train'\u001b[0m, \u001b[33mvalidation\u001b[0m=\u001b[32m'/opt/ml/input/data/validation'\u001b[0m, \n",
      "\u001b[33mnum_labels\u001b[0m=\u001b[1;36m2\u001b[0m, \u001b[33mmlm_probability\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.15\u001b[0m\u001b[1m)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Let's figure out the tokenizer to use, by looking at the mlm-pretraining job log.\n",
    "rprint(\n",
    "    \"ModelArguments(cache_dir='/tmp/transformers/cache', config_name=None, model_name_or_path='microsoft/layoutlm-base-uncased', model_revision='main', tokenizer_name=None, use_auth_token=False)\"\n",
    ")\n",
    "print()\n",
    "rprint(\n",
    "    \"DataTrainingArguments(annotation_attr='labels', max_seq_length=512, max_train_samples=None, task_name='mlm', textract='/opt/ml/input/data/textract', textract_prefix='textract-transformers/data/textracted', train='/opt/ml/input/data/train', validation='/opt/ml/input/data/validation', num_labels=2, mlm_probability=0.15)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a3a4c2-81e0-4f7b-b16f-5fc6139bec5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34523d91d9d3427ab77c583c285ad6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/170 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3199cd5517e04499abada2f7e104162e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/606 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29edb5c32ce42cc8579c9721a1e120a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c83b01bbda4957945a234e4f6e16d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a495b58e6e284687921d5f45a52c6390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|tokenization_utils_base.py:3325] 2022-01-18 11:06:22,348 >> Token indices sequence length is longer than the specified maximum sequence length for this model (924 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # For notebook only.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/layoutlm-base-uncased\",\n",
    "    cache_dir=datapaths_local.data_prefix / \"transformers/cache\",\n",
    "    use_fast=True,\n",
    "    revision=\"main\",\n",
    "    use_auth_token=False,\n",
    ")\n",
    "\n",
    "ds_mlm = mlm.TextractLayoutLMDatasetForLM(\n",
    "    textract_path=str(datapaths_local.textracted),\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a196d9f-78ff-4d28-ac5a-7e2828e8c8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m<\u001b[0m\u001b[1;95mmy_code.data.mlm.TextractLayoutLMDatasetForLM\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x7f88ca5c9eb0\u001b[0m\u001b[1m>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# TODO: GITROOT/notebooks/my_code/data/mlm.py:TextractLayoutLMDatasetForLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd6bd66-3f6d-4d51-ba5a-ebe15ead2229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (virtualenv_layoutlm-p39)",
   "language": "python",
   "name": "virtualenv_layoutlm-p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
